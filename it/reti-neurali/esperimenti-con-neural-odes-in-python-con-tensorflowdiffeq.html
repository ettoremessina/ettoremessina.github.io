<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="it">
	<head>
		<meta http-equiv="content-language" content="it">
		<meta name="author" content="Ettore Messina">

		<style>
			#cookiescript_checkbox_input {
				-moz-appearance: checkbox;
				-webkit-appearance: checkbox;
				-ms-appearance: checkbox;
				appearance: checkbox;
				opacity: 1.0;
			}
			#cookiescript_checkbox_text {
				color: white;
			}
			#cookiescript_description a:hover {
				color: yellow !important;
			}
		</style>
		<script type="text/javascript" charset="UTF-8" src="https://cookie-script.com/s/19e1626ea9f21a6fcc285b559b5957e6.js"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance" src="https://www.googletagmanager.com/gtag/js?id=UA-149444322-1"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance">
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'UA-149444322-1', { 'anonymize_ip': true });
		</script>


		<title>Esperimenti con Neural ODEs in Python con TensorFlowDiffEq</title>
		<meta name="description" content="Esempi d&apos;uso di Neural ODEs in Python con TensorFlow utilizzando il package TensorFlowDiffEq." >
		<meta name="keywords" content="Neural ODEs, equazioni differenziali ordinarie, approssimazione numerica, soluzione numerica, Python, TensorFlow, TensorFlowDiffEq" >
		<link rel="canonical" href="https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html" />
		<link rel="alternate" hreflang="en" href="https://computationalmindset.com/en/neural-networks/experiments-with-neural-odes-in-python-with-tensorflowdiffeq.html" />
		<link rel="alternate" hreflang="it" href="https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html" />
		
    <!-- SCHEMA.ORG JSON-LD WEBSITE -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "name": "Computational Mindset",
        "url": "https://computationalmindset.com/",
        "sameAs": ["https://www.facebook.com/ComputationalMindset/", "https://www.facebook.com/MentalitaComputazionale/", "https://github.com/ettoremessina/"],
        "author":
        {
          "@type": "Person",
          "name": "Ettore Messina",
          "image": "https://computationalmindset.com/images/ettore-messina.jpg",
          "gender": "Male",          
          "sameAs": ["https://www.facebook.com/ettore.messina.73/", "https://www.instagram.com/etmessina/", "https://twitter.com/ettoremessina/", "https://github.com/ettoremessina/", "https://medium.com/@ettoremessina/", "https://www.linkedin.com/in/ettoremessina/"]
        }
    }
    </script>

		
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement":
        [
		
          {
            "@type": "ListItem",
            "position": 1,
            "item":
            {
                "@id": "https://computationalmindset.com/it/",
                "name": "Mentalit&agrave; Computazionale"
            }
          },
		
          {
            "@type": "ListItem",
            "position": 2,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/",
              "name": "Reti Neurali"
            }
          },

          {
            "@type": "ListItem",
            "position": 3,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html",
              "name": "Esperimenti con Neural ODEs in Python con TensorFlowDiffEq"
            }
		  }
		
        ]
    }
    </script>
		

		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/railscasts.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
		
		<script>hljs.initHighlightingOnLoad();</script>
		<style>
			pre > code 
			{
				font-size: 1.2em;
			}
		</style>

		
		<script type="text/javascript" src="https://latex.codecogs.com/latexit.js"></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>



		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
		<link rel="icon" href="../../favicon.ico" type="image/x-icon" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../it/info.html" class="logo"><strong>Mentalit&agrave;&nbsp;Computazionale</strong> di&nbsp;Ettore&nbsp;Messina</a>
									<div style="text-align:right">
										<a class="logo" href="../../en/">en</a>
										&nbsp;&nbsp;&nbsp;
										<a class="logo" href="../../it/">it</a>
									</div>
									<ul class="icons">
										<li><a href="https://github.com/ettoremessina/" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.facebook.com/MentalitaComputazionale/" class="icon brands fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.youtube.com/channel/UCKrOtSEJjs5msOhPIdYEeWA/" class="icon brands fa-youtube" target="_blank"><span class="label">YouTube</span></a></li>
										<li><a href="https://www.linkedin.com/in/ettoremessina/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://medium.com/@ettoremessina/" class="icon brands fa-medium-m" target="_blank"><span class="label">Medium</span></a></li>
										<li><a href="https://linktr.ee/ComputationalMindset/" class="fas fa-link" style="color: grey;" target="_blank"><span class="label"></span></a></li>
									</ul>
								</header>

<!-- Content -->
	<section>
		<header class="main">
			<h1>Esperimenti con Neural ODEs in Python con TensorFlowDiffEq</h1>
		</header>
		<p>
            <a href="https://arxiv.org/abs/1806.07366" target="_blank"><em>Neural Ordinary Differential Equations</em></a> (abbreviato <em>Neural ODEs</em>) &egrave; un paper che introduce una nuova famiglia di reti neurali
            in cui alcuni strati nascosti (o anche l&apos;unico strato nei casi pi&ugrave; semplici) sono implementati con un risolutore di equazioni differenziali ordinarie. <br />
            Questo post mostra un esempio scritto in Python con TensorFlowDiffEq (in futuro ce ne saranno altri) che utilizza alcune idee descritte nel paper <em>Neural ODEs</em> per risolvere un problema di approssimazione della mappatura
            tra un input e un output in uno preciso scenario.<br />
            <br />
            Tutti i vari frammenti di codice descritti in questo post richiedono la versione 3.x di Python, TensorFlow 2.x, il package TensorFlowDiffEq e i seguenti package: NumPy, MatPlotLib.<br/>
			Per ottenere il codice si veda il paragrafo <a href='#downloadcode'>Download del codice completo</a> in fondo a questo post.<br/>
            Se si fosse interessati a vedere la soluzione degli stessi problemi in Julia si veda il post <a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-julia.html">Esperimenti con Neural ODEs in Julia</a> su questo sito web.<br />
            <br/>
        </p>

		<h2>Convenzioni</h2>
		<p>
            In questo post le convenzioni adoperate sono le seguenti:
            <ul>
                <li>$t$ &egrave; la variabile indipendente</li>
                <li>$x$ &egrave; la funzione incognita</li>
                <li>$y$ &egrave; la seconda funzione incognita</li>
                <li>
                $x$ e $y$ sono da intendersi funzioni di $t$, quindi $x=x(t)$ e $y=y(t)$,
                ma l&apos;uso di questa notazione compatta, oltre ad avere una maggiore leggibilit&agrave; a livello matematico
                rende pi&ugrave; agevole la "traduzione" in codice dell&apos;equazione
                </li>
                <li>$x'$ &egrave; la derivata prima di x rispetto a $t$ e naturalmente $y'$ &egrave; la derivata prima di y rispetto a $t$</li>
                <!--<li>$x''$ &egrave; la derivata seconda di x rispetto a $t$ e naturalmente $y''$ &egrave; la derivata seconda di y rispetto a $t$</li>-->
            </ul>
        </p>
        <br />

		<h2 id="exp1">Esperimento #1: addestrare un sistema di ODE per soddisfare un obiettivo</h2>
        <p>
            Un percettrone multistrato (abbreviato MLP) &egrave; uno strumento opportuno per imparare una relazione non lineare tra input e output di cui non si conosce la legge.<br />
            Ci sono casi invece in cui si ha conoscenza a priori della legge che correla gli input e gli output, ad esempio nella forma di un sistema parametrico di equazioni differenziali:
            in questa situazione una rete neurale di tipo MLP non consente di utilizzare tale conoscenza mentre una rete di tipo Neural ODEs s&igrave;.<br />
        </p>
 
 		<h3>Lo scenario di applicazione</h3>
        <p>
            Lo scenario di applicazione &egrave; il seguente:<br />
            <ul></ul>
            <b>Conoscenza a priori:</b>
            <ul>
                <li>Un dataset che contenga gli input e gli output</li>
                <li>Una legge che associ input e output in forma di sistema parametrico di equazioni differenziali</li>
            </ul>
            <b>Obiettivo:</b>
            <ul>
                <li>Determinare opportuni valori dei parametri affiché il sistema ottenuto sostituendo i parametri formali con i valori determinati approssimi al meglio la mappatura tra input e output.</li>
            </ul>
            Lo stesso scenario &egrave; stato applicato nel repository <a href="https://github.com/mandubian/neural-ode/" target="_blank">neural-ode</a>
            di <em>mandubian</em> su GitHub ove la legge &egrave; il sistema di equazioni di Lotka-Volterra che descrive la dinamica della popolazione di prede e predatori.<br />
            In questo post la legge non &egrave; volutamente una legge famosa, ma &egrave; il sistema parametrico di due equazioni del primo ordine e otto parametri mostrato nel seguente paragrafo.
        </p>

 		<h3>Il problema da risolvere</h3>
        <p>
            Sia dato il seguente sistema parametrico di due equazioni differenziali ordinarie con valori iniziali
            che rappresenta la legge che descrive il comportamento di un ipotetico sistema dinamico:
            
            $$ \begin{equation}
            \begin{cases}
                x' = a_1x + b_1y + c_1e^{-d_1t}
                \\
                y'= a_2x + b_2y + c_2e^{-d_2t}
                \\ 
                x(0)=0
                \\
                y(0)=0
            \end{cases}
            \end{equation} $$

            Ovviamente questa &egrave; una demo il cui scopo &egrave; provare la bont&agrave; del metodo, quindi per preparare il dataset
            fissiamo arbitrariamente gli otto valori dei parametri, ad esempio questi:
            
        $$ \left[\begin{matrix}a_1 \\ b_1 \\ c_1 \\ d_1 \\ a_2 \\ b_2 \\ c_2 \\ d_2 \end{matrix} \right] = 
           \left[\begin{matrix}1.11 \\ 2.43 \\ -3.66 \\ 1.37 \\ 2.89 \\ -1.97 \\ 4.58 \\ 2.86 \end{matrix} \right] $$

            e sapendo che con tali valori dei parametri la soluzione analitica &egrave; la seguente:
            
            $$ \begin{equation}
                \begin{array}{l}
                x(t) = \\
                \;\; -1.38778 \cdot 10^{-17} \; e^{-8.99002 t} - \\
                \;\; 2.77556 \cdot 10^{-17} \; e^{-7.50002 t} + \\
                \;\; 3.28757 \; e^{-3.49501 t} - \\
                \;\; 3.18949  \; e^{-2.86 t} + \\
                \;\; 0.258028 \; e^{-1.37 t} - \\
                \;\; 0.356108 \; e^{2.63501 t} + \\
                \;\; 4.44089 \cdot 10^{-16} \; e^{3.27002 t} + \\
                \;\; 1.11022 \cdot 10^{-16} \; e^{4.76002 t} \\
                \\
                y(t) = \\
                \;\; -6.23016 \; e^{-3.49501 t} + \\
                \;\; 5.21081 \; e^{-2.86 t} + \\
                \;\; 1.24284 \; e^{-1.37 t} - \\
                \;\; 0.223485 \; e^{2.63501 t} + \\
                \;\; 2.77556 \cdot 10^{-17} \; e^{4.76002 t} \\
                \end{array}
            \end{equation} $$

            (verificabile online tramite <a href="https://www.wolframalpha.com/input/?i=x%27+%3D+1.11+x+%2B+2.43+y+%2B+-3.66+e%5E%28-1.37+t%29%3B+y%27+%3D+2.89+x+-1.97+y+%2B+4.58+e%5E+%28-2.86+t%29%3B+x%280%29%3D0%3B+y%280%29%3D0%3B
" target="_brank">Wolfram Alpha</a>)
            si &egrave; in grado di preparare il dataset: l&apos;input &egrave; un intervallo discretizzato del tempo da $0$ a $1.5$ passo $0.01$,
            mentre l&apos;output &egrave; costituito dalle soluzioni analitiche $x=x(t)$ e $y=y(t)$ per ogni $t$ appartenente all&apos;input.<br />
            Una volta preparato il dataset ci si dimentichi dei valori dei parametri e della soluzione analitica e ci si pone il problema
            di come addestrare una rete neurale per determinare un opportuno set di valori per gli otto parametri per approssimare al meglio
            la mappatura non lineare tra input e output del dataset.<br />            
            <br />
        </p>

 		<h3>L&apos;implementazione della soluzione</h3>
        <p>
            Il sistema parametrico di equazioni differenziali &egrave; gi&agrave; scritto in forma esplicita e in Python con TensorFlowDiffEq si implementa cos&igrave;:
            <pre><code class="python">def parametric_ode_system(t, u, args):
    a1, b1, c1, d1, a2, b2, c2, d2 = \
        args[0], args[1], args[2], args[3], \
        args[4], args[5], args[6], args[7]
    x, y = u[0], u[1]
    dx_dt = a1*x + b1*y + c1*tf.math.exp(-d1*t)
    dy_dt = a2*x + b2*y + c2*tf.math.exp(-d2*t)
    return tf.stack([dx_dt, dy_dt])</code></pre>
            I setting sono:
            <ul>
                <li>Input: $t \in [0, 1.5]$ passo di discretizzazione $0.01$</li>
                <li>Condizioni al contorno: $x(0)=0; y(0)=0$</li>
                <li>Valori iniziali del parametri qualsiasi; per comodit&agrave; qui si impostano tutti uguali a $1$</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="python">t_begin=0.
t_end=1.5
t_nsamples=150
t_space = np.linspace(t_begin, t_end, t_nsamples)
t_space_tensor = tf.constant(t_space)
x_init = tf.constant([0.], dtype=t_space_tensor.dtype)
y_init = tf.constant([0.], dtype=t_space_tensor.dtype)
u_init = tf.convert_to_tensor([x_init, y_init], dtype=t_space_tensor.dtype)
args = [tf.Variable(initial_value=1., name='p' + str(i+1), trainable=True,
          dtype=t_space_tensor.dtype) for i in range(0, 8)]</code></pre>

            La rete neurale &egrave; costituita da un solo strato di tipo <em>ODE solver</em>:<br />
            <br />
            <pre><code class="python">def net():
  return odeint(lambda ts, u0: parametric_ode_system(ts, u0, args),
                  u_init, t_space_tensor)</code></pre>
            <br />

            Le impostazioni dell&apos;addrestramento sono:<br />
            <ul>
                <li>Ottimizzatore: Adam</li>
                <li>Learning rate: $0.05$</li>
                <li>Numero di epoche: $200$</li>
                <li>Funzione di loss: somma dei quadrati delle differenze</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="python">learning_rate = 0.05
epochs = 200
optimizer = tfko.Adam(learning_rate=learning_rate)

def loss_func(num_sol):
  return tf.reduce_sum(tf.square(dataset_outs[0] - num_sol[:, 0])) + \
         tf.reduce_sum(tf.square(dataset_outs[1] - num_sol[:, 1]))  

for epoch in range(epochs):
  with tf.GradientTape() as tape:
    num_sol = net()
    loss_value = loss_func(num_sol)
  print("Epoch:", epoch, " loss:", loss_value.numpy())
  grads = tape.gradient(loss_value, args)
  optimizer.apply_gradients(zip(grads, args))</code></pre>
            <br />
            Qui di seguito il codice completo:</br >
            </br >
            <pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow.keras.optimizers as tfko
from tfdiffeq import odeint

def parametric_ode_system(t, u, args):
    a1, b1, c1, d1, a2, b2, c2, d2 = \
        args[0], args[1], args[2], args[3], \
        args[4], args[5], args[6], args[7]
    x, y = u[0], u[1]
    dx_dt = a1*x + b1*y + c1*tf.math.exp(-d1*t)
    dy_dt = a2*x + b2*y + c2*tf.math.exp(-d2*t)
    return tf.stack([dx_dt, dy_dt])

true_params = [1.11, 2.43, -3.66, 1.37, 2.89, -1.97, 4.58, 2.86]

an_sol_x = lambda t : \
  -1.38778e-17 * np.exp(-8.99002 * t) - \
  2.77556e-17 * np.exp(-7.50002 * t) + \
  3.28757 * np.exp(-3.49501 * t) - \
  3.18949 * np.exp(-2.86 * t) + \
  0.258028 * np.exp(-1.37 * t) - \
  0.356108 * np.exp(2.63501 * t) +  \
  4.44089e-16 * np.exp(3.27002 * t) + \
  1.11022e-16 * np.exp(4.76002 * t)

an_sol_y = lambda t : \
  -6.23016 * np.exp(-3.49501 * t) + \
  5.21081 * np.exp(-2.86 * t) + \
  1.24284 * np.exp(-1.37 * t) - \
  0.223485 * np.exp(2.63501 * t) + \
  2.77556e-17 * np.exp(4.76002 * t)

t_begin=0.
t_end=1.5
t_nsamples=150
t_space = np.linspace(t_begin, t_end, t_nsamples)

dataset_outs = [tf.expand_dims(an_sol_x(t_space), axis=1), \
                tf.expand_dims(an_sol_y(t_space), axis=1)]

t_space_tensor = tf.constant(t_space)
x_init = tf.constant([0.], dtype=t_space_tensor.dtype)
y_init = tf.constant([0.], dtype=t_space_tensor.dtype)
u_init = tf.convert_to_tensor([x_init, y_init], dtype=t_space_tensor.dtype)
args = [tf.Variable(initial_value=1., name='p' + str(i+1), trainable=True,
          dtype=t_space_tensor.dtype) for i in range(0, 8)]

learning_rate = 0.05
epochs = 200
optimizer = tfko.Adam(learning_rate=learning_rate)

def net():
  return odeint(lambda ts, u0: parametric_ode_system(ts, u0, args),
                  u_init, t_space_tensor)

def loss_func(num_sol):
  return tf.reduce_sum(tf.square(dataset_outs[0] - num_sol[:, 0])) + \
         tf.reduce_sum(tf.square(dataset_outs[1] - num_sol[:, 1]))

for epoch in range(epochs):
  with tf.GradientTape() as tape:
    num_sol = net()
    loss_value = loss_func(num_sol)
  print("Epoch:", epoch, " loss:", loss_value.numpy())
  grads = tape.gradient(loss_value, args)
  optimizer.apply_gradients(zip(grads, args))

print("Learned parameters:", [args[i].numpy() for i in range(0, 4)])
num_sol = net()
x_num_sol = num_sol[:, 0].numpy()
y_num_sol = num_sol[:, 1].numpy()

x_an_sol = an_sol_x(t_space)
y_an_sol = an_sol_y(t_space)

plt.figure()
plt.plot(t_space, x_an_sol,'--', linewidth=2, label='analytical x')
plt.plot(t_space, y_an_sol,'--', linewidth=2, label='analytical y')
plt.plot(t_space, x_num_sol, linewidth=1, label='numerical x')
plt.plot(t_space, y_num_sol, linewidth=1, label='numerical y')
plt.title('Neural ODEs to fit params')
plt.xlabel('t')
plt.legend()
plt.show()</code></pre>
            Qui il link al codice su <a target="_blank" href="https://github.com/ettoremessina/differential-equations/blob/main/ODEs/neural-odes-demos/python/TensorFlowDiffEq/train-sys-of-odes-to-meet-objective.py">GitHub</a>.<br />
            <br />
            I valori dei parametri ottenuti al termine dell&apos;addrestramento sono i seguenti:
            
        $$ \left[\begin{matrix}a_1 \\ b_1 \\ c_1 \\ d_1 \\ a_2 \\ b_2 \\ c_2 \\ d_2 \end{matrix} \right] = 
           \left[\begin{matrix}1.5526739031012387 \\ 1.153349483970675 \\ -1.705896205847302 \\ 0.38685266435358123 \\ 
                               1.0173442364137184 \\ 0.7136534371585501 \\ -0.9789359917976935 \\ 1.4237334700663127 \end{matrix} \right] $$

            che sono ovviamente diversi dai valori dei parametri utilizzati per generare il dataset (passando tramite la soluzione analitica);
            per&ograve; il sistema di equazioni ottenuto sostituendo i parametri formali con tali valori ottenuti dall&apos;addestramento della rete neurale
            e risolvendolo numericamente questo ultimo sistema si ottiene una soluzione numerica che approssima abbastanza bene il dataset, come mostrato dalla figura:<br />
            <br />
        </p>
        <div class="betweentextlines"><img src="../../posts/neural-networks/experiments-with-neural-odes-in-python-with-tensorflowdiffeq/nn-expnodestfde-exp-1-result.png" /></div>
        <div class="photocaption">Comparazione della soluzione numerica che approssima il dataset.</div>
        <br />
        <p>
            Differenti strategie di addestramento consentono di ottenere diversi valori dei parametri e quindi una diversa soluzione numerica del sistema
            con una differente accuratezza. 
        </p>

        <h2>Citazioni</h2>
        <p>
            <pre><code class="text">@articleDBLP:journals/corr/abs-1806-07366,
  author    = {Tian Qi Chen and
               Yulia Rubanova and
               Jesse Bettencourt and
               David Duvenaud},
  title     = {Neural Ordinary Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1806.07366},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.07366},
  archivePrefix = {arXiv},
  eprint    = {1806.07366},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-07366.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
</code></pre>
        </p>

        <h2 id="downloadcode">Download del codice completo</h2>
		<p>
			Il codice completo &egrave; disponibile su <a target="_blank" href="https://github.com/ettoremessina/differential-equations/tree/main/ODEs/neural-odes-demos/python/TensorFlowDiffEq/">GitHub</a>.
			<br/>
			
			Questo materiale &egrave; distribuito su licenza MIT; sentiti libero di usare, condividere, &quot;forkare&quot; e adattare tale materiale come credi.
			<br/>
			Sentiti anche libero di pubblicare pull-request e bug-report su questo repository di GitHub oppure di contattarmi sui miei canali social disponibili nell&apos;angolo in alto a destra di questa pagina. 
			<br/>

		</p>

	</section>

						</div>
				</div>
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<div class="align-center"><img src="../../images/cm-logo-small.png" alt="Mentalit&agrave;&nbsp;Computazionale"/></div>
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="../../it/">Home</a></li>
										<li>
											<span class="opener">Reti&nbsp;Neurali</span>
											<ul>
												<li><a href="../../it/reti-neurali/">INDICE</a></li>
												<li><a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html">Esperimenti con Neural ODEs in Python con TensorFlowDiffEq</a></li>
												<li><a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-julia.html">Esperimenti con Neural ODEs in Julia</a></li>
												<li><a href="../../it/reti-neurali/risolutori-di-equazioni-differenziali-ordinarie.html">Risolutori di equazioni differenziali ordinarie in Python</a></li>
												<li><a href="../../it/reti-neurali/risolutori-di-equazioni-differenziali-ordinarie-in-julia.html">Risolutori di equazioni differenziali ordinarie in Julia</a></li>
												<li><a href="../../it/reti-neurali/forecast-di-una-serie-temporale-univariata-equispaziata-con-tensorflow.html">Forecast di una serie temporale univariata ed equispaziata con TensorFlow</a></li>
												<li><a href="../../it/reti-neurali/approssimazione-con-percettroni-multistrato-altamente-configurabili.html">Approssimazione con percettroni multistrato altamente configurabili</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Computazione&nbsp;Quantistica</span>
											<ul>
												<li><a href="../../it/computazione-quantistica/">INDICE</a></li>
												<li><a href="../../it/computazione-quantistica/operatori-not-cnot.html">Porte quantistiche NOT e C-NOT</a></li>
												<li><a href="../../it/computazione-quantistica/generazione-numero-casuale.html">Generazione di un numero casuale</a></li>
												<li><a href="../../it/computazione-quantistica/porte-hadamard-in-cascata.html">Porte Hadamard in cascata</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Machine&nbsp;Learning</span>
											<ul>
												<li><a href="../../it/machine-learning/">INDICE</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html">Approssimazione di funzioni tramite un regressore XGBoost configurabile</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-svr-configurabile.html">Approssimazione di funzioni tramite un Support Vector Regressor configurabile</a></li>
												<li><a href="../../it/machine-learning/regressione-polinomiale-con-accord-net.html">Regressione polinomiale con Accord.NET</a></li>
												<li><a href="../../it/machine-learning/regressione-smo-con-kernel-puk-in-weka.html">Regressione con SMO per SVM con kernel PUK in Weka</a></li>
												<li><a href="../../it/machine-learning/forecast-smo-con-kernel-polinomiale-in-weka.html">Forecast con SMO per SVM con kernel polinomiale in Weka</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Matematica</span>
											<ul>
												<li><a href="../../it/matematica/">INDICE</a></li>
												<li><a href="../../it/matematica/esperimenti-con-sympy-per-risolvere-odes-ordine-1.html">Esperimenti con SymPy per risolvere equazioni differenziali ordinarie del 1&deg; ordine</a></li>
												<li><a href="../../it/matematica/metodo-soluzione-dde-primo-ordine-usando-funzione-w-lambert.html">Un metodo di soluzione di una equazione differenziale con ritardo del primo ordine utilizzando la funzione W di Lambert</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Dataset</span>
											<ul>
												<li><a href="../../it/datasets/">INDICE</a></li>
												<li><a href="../../it/datasets/functions-dataset.html">Collezione di dataset &apos;Functions&apos;</a></li>
												<li><a href="../../it/datasets/time-series-dataset.html">Collezione di dataset &apos;Time&nbsp;Series&apos;</a></li>
												<li><a href="../../it/datasets/synthetic-words-dataset.html">Dataset &apos;Synthetic Words&apos;</a></li>
											</ul>
										</li>
										<li><a href="../../it/info.html">Info</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
                                    <div class="side-topics">
                                        <header class="align-center">
                                            <h2><a href="../../it/reti-neurali/">Reti&nbsp;Neurali</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/reti-neurali/" class="image"><span class="icon solid fa-sitemap"/></a>
                                        </article>
                                        <header class="align-center">
                                            <h2><a href="../../it/computazione-quantistica/">Computazione&nbsp;Quantistica</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/computazione-quantistica/" class="image"><span class="icon solid fa-atom"/></a>
                                        </article>
                                    </div>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">
										Grafica basata sul template &apos;Editorial&apos; (con personalizzazione) scaricato da <a href="https://html5up.net" target="_blank">HTML5 UP</a>.
										<br/>
										Clicka sui link per vedere i file <a href="../../html5up-license/LICENSE.txt" target="_blank">LICENSE.txt</a> e <a href="../../html5up-license/README.txt" target="_blank">README.txt</a> del template &apos;Editorial&apos; di HTML5 UP.
										<br>
										<br>
										&copy; <a href="../../it/info.html">Ettore Messina</a>. 
									</p>
								</footer>
						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

			<style>
				a.cc-link
				{
			    	border-bottom: none;
				}
				a.cc-link:hover
				{
					color: white !important;
				}
			</style>
	</body>
</html>


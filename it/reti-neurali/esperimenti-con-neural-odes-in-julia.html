
dnldnl
<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="it">
	<head>
		<meta http-equiv="content-language" content="it">
		<meta name="author" content="Ettore Messina">

		<style>
			#cookiescript_checkbox_input {
				-moz-appearance: checkbox;
				-webkit-appearance: checkbox;
				-ms-appearance: checkbox;
				appearance: checkbox;
				opacity: 1.0;
			}
			#cookiescript_checkbox_text {
				color: white;
			}
			#cookiescript_description a:hover {
				color: yellow !important;
			}
		</style>
		<script type="text/javascript" charset="UTF-8" src="https://cookie-script.com/s/19e1626ea9f21a6fcc285b559b5957e6.js"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance" src="https://www.googletagmanager.com/gtag/js?id=UA-149444322-1"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance">
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'UA-149444322-1', { 'anonymize_ip': true });
		</script>


		<title>Esperimenti con Neural ODEs in Julia</title>
		<meta name="description" content="Esempi d&apos;uso di Neural ODEs in Julia utilizzando i package DifferentialEquations, Flux, DiffEqFlux dell&apos;ecosistema di Julia." >
		<meta name="keywords" content="Neural ODEs, equazioni differenziali ordinarie, approssimazione numerica, soluzione numerica, forecast, serie temporali, Julia, DifferentialEquations, Flux, DiffEqFlux" >
		<link rel="canonical" href="https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-julia.html" />
		<link rel="alternate" hreflang="en" href="https://computationalmindset.com/en/neural-networks/experiments-with-neural-odes-in-julia.html" />
		<link rel="alternate" hreflang="it" href="https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-julia.html" />
		
    <!-- SCHEMA.ORG JSON-LD WEBSITE -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "name": "Computational Mindset",
        "url": "https://computationalmindset.com/",
        "sameAs": ["https://www.facebook.com/ComputationalMindset/", "https://www.facebook.com/MentalitaComputazionale/", "https://github.com/ettoremessina/"],
        "author":
        {
          "@type": "Person",
          "name": "Ettore Messina",
          "image": "https://computationalmindset.com/images/ettore-messina.jpg",
          "gender": "Male",          
          "sameAs": ["https://www.facebook.com/ettore.messina.73/", "https://www.instagram.com/etmessina/", "https://twitter.com/ettoremessina/", "https://github.com/ettoremessina/", "https://medium.com/@ettoremessina/", "https://www.linkedin.com/in/ettoremessina/"]
        }
    }
    </script>

		
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement":
        [
		
          {
            "@type": "ListItem",
            "position": 1,
            "item":
            {
                "@id": "https://computationalmindset.com/it/",
                "name": "Mentalit&agrave; Computazionale"
            }
          },
		
          {
            "@type": "ListItem",
            "position": 2,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/",
              "name": "Reti Neurali"
            }
          },

          {
            "@type": "ListItem",
            "position": 3,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/esperimenti-con-neural-odes-in-julia.html",
              "name": "Esperimenti con Neural ODEs in Julia"
            }
		  }
		
        ]
    }
    </script>
		

		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/railscasts.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
		
		<script>hljs.initHighlightingOnLoad();</script>
		<style>
			pre > code 
			{
				font-size: 1.2em;
			}
		</style>

		
		<script type="text/javascript" src="https://latex.codecogs.com/latexit.js"></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>



		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
		<link rel="icon" href="../../favicon.ico" type="image/x-icon" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../it/info.html" class="logo"><strong>Mentalit&agrave;&nbsp;Computazionale</strong> di&nbsp;Ettore&nbsp;Messina</a>
									<div style="text-align:right">
										<a class="logo" href="../../en/">en</a>
										&nbsp;&nbsp;&nbsp;
										<a class="logo" href="../../it/">it</a>
									</div>
									<ul class="icons">
										<li><a href="https://github.com/ettoremessina/" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.facebook.com/MentalitaComputazionale/" class="icon brands fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.youtube.com/channel/UCKrOtSEJjs5msOhPIdYEeWA/" class="icon brands fa-youtube" target="_blank"><span class="label">YouTube</span></a></li>
										<li><a href="https://www.linkedin.com/in/ettoremessina/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://medium.com/@ettoremessina/" class="icon brands fa-medium-m" target="_blank"><span class="label">Medium</span></a></li>
										<li><a href="https://linktr.ee/ComputationalMindset/" class="fas fa-link" style="color: grey;" target="_blank"><span class="label"></span></a></li>
									</ul>
								</header>

<!-- Content -->
    $\newcommand\dag\dagger$
	<section>
		<header class="main">
			<h1>Esperimenti con Neural ODEs in Julia</h1>
		</header>
		<p>
            <a href="https://arxiv.org/abs/1806.07366" target="_blank"><em>Neural Ordinary Differential Equations</em></a> (abbreviato <em>Neural ODEs</em>) &egrave; un paper che introduce una nuova famiglia di reti neurali
            in cui alcuni strati nascosti (o anche l&apos;unico strato nei casi pi&ugrave; semplici) sono implementati con un risolutore di equazioni differenziali ordinarie. <br />
            Questo post mostra due esempi scritti in Julia (in futuro ce ne saranno altri) che utilizzano alcune idee descritte nel paper <em>Neural ODEs</em> per mostrare possibili soluzioni nei seguenti scenari:
            <ul>
                <li><a href="#exp1">Esperimento #1</a>: addestrare un sistema di ODE per soddisfare un obiettivo.</li>
                <li><a href="#exp2">Esperimento #2</a>: calcolare il forecast di un sistema di serie temporali descritte da una legge differenziale.</li>
            <br />
            Tutti i vari frammenti di codice descritti in questo post richiedono la versione 1.5.3 di Julia e i seguenti package: DifferentialEquations, Flux, DiffEqFlux, Plots.<br/>
			Per ottenere il codice si veda il paragrafo <a href='#downloadcode'>Download del codice completo</a> in fondo a questo post.<br/>
            Se si fosse interessati a vedere la soluzione degli stessi problemi in Python con TensorFlow si veda il post <a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html">Esperimenti con Neural ODEs in Python con TensorFlowDiffEq</a> su questo sito web.<br />
			<br/>
        </p>

		<h2>Convenzioni</h2>
		<p>
            In questo post le convenzioni adoperate sono le seguenti:
            <ul>
                <li>$t$ &egrave; la variabile indipendente</li>
                <li>$x$ &egrave; la funzione incognita</li>
                <li>$y$ &egrave; la seconda funzione incognita</li>
                <li>
                $x$ e $y$ sono da intendersi funzioni di $t$, quindi $x=x(t)$ e $y=y(t)$,
                ma l&apos;uso di questa notazione compatta, oltre ad avere una maggiore leggibilit&agrave; a livello matematico
                rende pi&ugrave; agevole la "traduzione" in codice dell&apos;equazione
                </li>
                <li>$x'$ &egrave; la derivata prima di x rispetto a $t$ e naturalmente $y'$ &egrave; la derivata prima di y rispetto a $t$</li>
                <!--<li>$x''$ &egrave; la derivata seconda di x rispetto a $t$ e naturalmente $y''$ &egrave; la derivata seconda di y rispetto a $t$</li>-->
            </ul>
        </p>
        <br />

		<h2 id="exp1">Esperimento #1: addestrare un sistema di ODE per soddisfare un obiettivo</h2>
        <p>
            Un percettrone multistrato (abbreviato MLP) &egrave; uno strumento opportuno per imparare una relazione non lineare tra input e output di cui non si conosce la legge.<br />
            Ci sono casi invece in cui si ha conoscenza a priori della legge che correla gli input e gli output, ad esempio nella forma di un sistema parametrico di equazioni differenziali:
            in questa situazione una rete neurale di tipo MLP non consente di utilizzare tale conoscenza mentre una rete di tipo Neural ODEs s&igrave;.<br />
        </p>
 
 		<h3>Lo scenario di applicazione</h3>
        <p>
            Lo scenario di applicazione &egrave; il seguente:<br />
            <ul></ul>
            <b>Conoscenza a priori:</b>
            <ul>
                <li>Un dataset che contenga gli input e gli output.</li>
                <li>Una legge che associ input e output in forma di sistema parametrico di equazioni differenziali.</li>
            </ul>
            <b>Obiettivo:</b>
            <ul>
                <li>Determinare opportuni valori dei parametri affich&eacute; il sistema ottenuto sostituendo i parametri formali con i valori determinati approssimi al meglio la mappatura tra input e output.</li>
            </ul>
            Lo stesso scenario &egrave; stato applicato nel post <a href="https://julialang.org/blog/2019/01/fluxdiffeq/#lets_put_an_ode_into_a_neural_net_framework" target="_blank">DiffEqFlux.jl – A Julia Library for Neural Differential Equations</a>
            sul blog ufficiale di Julia ove la legge &egrave; il sistema di equazioni di Lotka-Volterra che descrive la dinamica della popolazione di prede e predatori.<br />
            In questo post la legge non &egrave; volutamente una legge famosa, ma &egrave; il sistema parametrico di due equazioni del primo ordine e otto parametri mostrato nel seguente paragrafo.
        </p>

 		<h3>Il problema da risolvere</h3>
        <p>
            Sia dato il seguente sistema parametrico di due equazioni differenziali ordinarie con valori iniziali
            che rappresenta la legge che descrive il comportamento di un ipotetico sistema dinamico:
            
            $$ \begin{equation}
            \begin{cases}
                x' = a_1x + b_1y + c_1e^{-d_1t}
                \\
                y'= a_2x + b_2y + c_2e^{-d_2t}
                \\ 
                x(0)=0
                \\
                y(0)=0
            \end{cases}
            \end{equation} $$

            Ovviamente questa &egrave; una demo il cui scopo &egrave; provare la bont&agrave; del metodo, quindi per preparare il dataset
            si fissino arbitrariamente gli otto valori dei parametri, ad esempio questi:
            
        $$ \left[\begin{matrix}a_1 \\ b_1 \\ c_1 \\ d_1 \\ a_2 \\ b_2 \\ c_2 \\ d_2 \end{matrix} \right] = 
           \left[\begin{matrix}1.11 \\ 2.43 \\ -3.66 \\ 1.37 \\ 2.89 \\ -1.97 \\ 4.58 \\ 2.86 \end{matrix} \right] $$

            e sapendo che con tali valori dei parametri la soluzione analitica &egrave; la seguente:
            
            $$ \begin{equation}
                \begin{array}{l}
                x(t) = \\
                \;\; -1.38778 \cdot 10^{-17} \; e^{-8.99002 t} - \\
                \;\; 2.77556 \cdot 10^{-17} \; e^{-7.50002 t} + \\
                \;\; 3.28757 \; e^{-3.49501 t} - \\
                \;\; 3.18949  \; e^{-2.86 t} + \\
                \;\; 0.258028 \; e^{-1.37 t} - \\
                \;\; 0.356108 \; e^{2.63501 t} + \\
                \;\; 4.44089 \cdot 10^{-16} \; e^{3.27002 t} + \\
                \;\; 1.11022 \cdot 10^{-16} \; e^{4.76002 t} \\
                \\
                y(t) = \\
                \;\; -6.23016 \; e^{-3.49501 t} + \\
                \;\; 5.21081 \; e^{-2.86 t} + \\
                \;\; 1.24284 \; e^{-1.37 t} - \\
                \;\; 0.223485 \; e^{2.63501 t} + \\
                \;\; 2.77556 \cdot 10^{-17} \; e^{4.76002 t} \\
                \end{array}
            \end{equation} $$

            (verificabile online tramite <a href="https://www.wolframalpha.com/input/?i=x%27+%3D+1.11+x+%2B+2.43+y+%2B+-3.66+e%5E%28-1.37+t%29%3B+y%27+%3D+2.89+x+-1.97+y+%2B+4.58+e%5E+%28-2.86+t%29%3B+x%280%29%3D0%3B+y%280%29%3D0%3B
" target="_brank">Wolfram Alpha</a>)
            si &egrave; in grado di preparare il dataset: l&apos;input &egrave; un intervallo discretizzato del tempo da $0$ a $1.5$ passo $0.01$,
            mentre l&apos;output &egrave; costituito dalle soluzioni analitiche $x=x(t)$ e $y=y(t)$ per ogni $t$ appartenente all&apos;input.<br />
            Una volta preparato il dataset ci si dimentichi dei valori dei parametri e della soluzione analitica e ci si pone il problema
            di come addestrare una rete neurale per determinare un opportuno set di valori per gli otto parametri per approssimare al meglio
            la mappatura non lineare tra input e output del dataset.<br />            
            <br />
        </p>

 		<h3>L&apos;implementazione della soluzione</h3>
        <p>
            Il sistema parametrico di equazioni differenziali &egrave; gi&agrave; scritto in forma esplicita e in Julia con DifferentialEquations si implementa cos&igrave;:
            <pre><code class="julia">function parametric_ode_system!(du,u,p,t)
  x, y = u
  a1, b1, c1, d1, a2, b2, c2, d2 = p
  du[1] = dx = a1*x + b1*y + c1*exp(-d1*t)
  du[2] = dy = a2*x + b2*y + c2*exp(-d2*t)
end</code></pre>
            I setting sono:
            <ul>
                <li>Input: $t \in [0, 1.5]$ passo di discretizzazione $0.01$</li>
                <li>Condizioni al contorno: $x(0)=0; y(0)=0$</li>
                <li>Valori iniziali del parametri qualsiasi; per comodit&agrave; qui si impostano tutti uguali a $1$</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="julia">tbegin=0.0
tend=1.5
tstep=0.01
trange = tbegin:tstep:tend
u0 = [0.0,0.0]
tspan = (tbegin,tend)
p = ones(8)</code></pre>

            La rete neurale &egrave; costituita da un solo strato di tipo <em>ODE solver</em>:<br />
            <br />
            <pre><code class="julia">prob = ODEProblem(parametric_ode_system!, u0, tspan, p)

function net()
    solve(prob, Tsit5(), p=p, saveat=trange)
end</code></pre>
            <br />

            Le impostazioni dell&apos;addrestramento sono:<br />
            <ul>
                <li>Ottimizzatore: ADAM</li>
                <li>Learning rate: $0.05$</li>
                <li>Numero di epoche: $1000$</li>
                <li>Funzione di loss: somma dei quadrati delle differenze</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="julia">epochs = 1000
learning_rate = 0.05
data = Iterators.repeated((), epochs)
opt = ADAM(learning_rate)
callback_func = function ()
  #.......
end
fparams = Flux.params(p)
Flux.train!(loss_func, fparams, data, opt, cb=callback_func)</code></pre>
            <br />
            Qui di seguito il codice completo:</br >
            </br >
            <pre><code class="julia">using Flux, DiffEqFlux, DifferentialEquations, Plots

function parametric_ode_system!(du,u,p,t)
  x, y = u
  a1, b1, c1, d1, a2, b2, c2, d2 = p
  du[1] = dx = a1*x + b1*y + c1*exp(-d1*t)
  du[2] = dy = a2*x + b2*y + c2*exp(-d2*t)
end

true_params = [1.11, 2.43, -3.66, 1.37, 2.89, -1.97, 4.58, 2.86]

an_sol_x(t) =
  -1.38778e-17 * exp(-8.99002 * t) -
  2.77556e-17 * exp(-7.50002 * t) +
  3.28757 * exp(-3.49501 * t) -
  3.18949 * exp(-2.86 * t) +
  0.258028 * exp(-1.37 * t) -
  0.356108 * exp(2.63501 * t) +
  4.44089e-16 * exp(3.27002 * t) +
  1.11022e-16 * exp(4.76002 * t)
an_sol_y(t) =
  -6.23016 * exp(-3.49501 * t) +
  5.21081 * exp(-2.86 * t) +
  1.24284 * exp(-1.37 * t) -
  0.223485 * exp(2.63501 * t) +
  2.77556e-17 * exp(4.76002 * t)

tbegin=0.0
tend=1.5
tstep=0.01
trange = tbegin:tstep:tend
u0 = [0.0,0.0]
tspan = (tbegin,tend)
p = ones(8)

prob = ODEProblem(parametric_ode_system!, u0, tspan, p)

function net()
    solve(prob, Tsit5(), p=p, saveat=trange)
end

dataset_outs = [an_sol_x.(trange), an_sol_y.(trange)]
function loss_func()
  pred = net()
  sum(abs2, dataset_outs[1] .- pred[1,:]) +
  sum(abs2, dataset_outs[2] .- pred[2,:])
end

epochs = 1000
learning_rate = 0.05
data = Iterators.repeated((), epochs)
opt = ADAM(learning_rate)
callback_func = function ()
  println("loss: ", loss_func())
end
fparams = Flux.params(p)
Flux.train!(loss_func, fparams, data, opt, cb=callback_func)

predict_prob = ODEProblem(parametric_ode_system!, u0, tspan, p)
predict_sol = solve(prob, Tsit5(), saveat=trange)
x_predict_sol = [u[1] for u in predict_sol.u]
y_predict_sol = [u[2] for u in predict_sol.u]

println("Learned parameters:", p)

plot(trange, dataset_outs[1],
    linewidth=2, ls=:dash,
    title="Neural ODEs to fit params",
    xaxis="t",
    label="dataset x(t)",
    legend=true)
plot!(trange, dataset_outs[2],
    linewidth=2, ls=:dash,
    label="dataset y(t)")
plot!(predict_sol.t, x_predict_sol,
    linewidth=1,
    label="predicted x(t)")
plot!(predict_sol.t, y_predict_sol,
    linewidth=1,
    label="predicted y(t)")
</code></pre>
            Qui il link al codice su <a target="_blank" href="https://github.com/ettoremessina/differential-equations/blob/main/ODEs/neural-odes-demos/julia/DiffEqFlux/train-sys-of-odes-to-meet-objective.jl">GitHub</a>.<br />
            <br />
            I valori dei parametri ottenuti al termine dell&apos;addrestramento sono i seguenti:
            
        $$ \left[\begin{matrix}a_1 \\ b_1 \\ c_1 \\ d_1 \\ a_2 \\ b_2 \\ c_2 \\ d_2 \end{matrix} \right] = 
           \left[\begin{matrix}1.7302980833638142 \\ 1.2823312512074032 \\ -1.6866178290795755 \\ 0.41974163099782325 \\ 
                               1.223075467559363 \\ 0.9410722500584323 \\ 0.18890958911958686 \\ 1.7462909509457183 \end{matrix} \right] $$

            che sono ovviamente diversi dai valori dei parametri utilizzati per generare il dataset (passando tramite la soluzione analitica);
            per&ograve; il sistema di equazioni ottenuto sostituendo i parametri formali con tali valori ottenuti dall&apos;addestramento della rete neurale
            e risolvendolo numericamente questo ultimo sistema si ottiene una soluzione numerica che approssima abbastanza bene il dataset, come mostrato dalla figura:<br />
            <br />
        </p>
        <div class="betweentextlines"><img src="../../posts/neural-networks/experiments-with-neural-odes-in-julia/nn-expnodesjl-exp-1-result.png" /></div>
        <div class="photocaption">Comparazione della soluzione numerica che approssima il dataset.</div>
        <br />
        <p>
            Differenti strategie di addestramento consentono di ottenere diversi valori dei parametri e quindi una diversa soluzione numerica del sistema
            con una differente accuratezza. 
        </p>
        <br />
        
		<h2 id="exp2">Esperimento #2: calcolare il forecast di un sistema di serie temporali descritte da una legge differenziale</h2>
        <p>
            Ci sono varie tassonomie di reti neurali black-box per calcolare il forecast di serie temporali (si veda il post si veda il post <a href="../../it/reti-neurali/forecast-di-una-serie-temporale-univariata-equispaziata-con-tensorflow.html">Forecast di una serie temporale univariata ed equispaziata con TensorFlow</a> su questo sito web)
            che sono adatte quando non si ha conoscenza a priori della legge matematica che descriva il comporamento delle serie temporali in input.<br />
            Quando invece si ha conoscenza a priori della legge differenziale che regoli l&apos;evoluzione delle derivate delle serie temporali
            (ad esempio nella forma di un sistema parametrico di equazioni differenziali)
            una rete di tipo Neural ODE diventa lo strumento giusto per calcolare il forecast in modo efficiente e accurato.<br />
        </p>

		<h3>Lo scenario di applicazione</h3>
        <p>
            Lo scenario di applicazione &egrave; il seguente:<br />
            <ul></ul>
            <b>Conoscenza a priori:</b>
            <ul>
                <li>Un dataset che contenga un set di serie temporali interdipendenti.</li>
                <li>Una legge che regoli l&apos;evoluzione delle derivate delle serie temporali nel tempo sotto forma di sistema parametrico di equazioni differenziali.</li>
            </ul>
            <b>Obiettivo:</b><br />
            L&apos; obiettivo &egrave; duplice:
            <ul>
                <li>1. Addestrare una rete neurale di tipo NeuralODE per imparare il valore dei parametri del sistema di ODE a partire dalle serie temporali in input.</li>
                <li>2. Utilizzare il sistema imparato per calcolare il forecast, quindi le predizioni con tempo posteriore a quello delle serie temporali iniziali.</li>
            </ul>
            L&apos;obiettivo 1 &egrave; simile a quello del post <a href="https://julialang.org/blog/2019/01/fluxdiffeq/#understanding_the_neural_ode_layer_behavior_by_example" target="_blank">DiffEqFlux.jl – A Julia Library for Neural Differential Equations</a>
            sul blog ufficiale di Julia ove la legge che regola le derivate delle serie temporali &egrave; $u(t) = A t^3$ con $A$ una matrice 2x2.<br />
            In questo post la legge &egrave; diversa ed &egrave; una legge di oscillazione con smorzamento e in pi&ugrave; &egrave; affrontato l&apos;obiettivo del forecast.
        </p>

 		<h3>Il problema da risolvere</h3>
        <p>
            Sia dato il seguente sistema parametrico di due equazioni differenziali ordinarie con valori iniziali
            che rappresenta la legge che descrive l&apos;evoluzione delle derivate di una coppia di serie temporali interdipendenti:
            
            $$ \begin{equation}
            \begin{cases}
                \left[\begin{matrix}
                x' & y'                
                \end{matrix} \right]
                =
                \left[\begin{matrix}
                \sin 2x + \cos 2y \\ \sin 2x + \cos 2y                
                \end{matrix} \right] ^ \dag
                \left[\begin{matrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix} \right]
                \\ 
                x(0)=x_0
                \\
                y(0)=y_0
            \end{cases}
            \end{equation} $$

            Ovviamente questa &egrave; una demo il cui scopo &egrave; provare la bont&agrave; del metodo, quindi per preparare il dataset
            si fissino arbitrariamente i valori dei quattro parametri e le due condizioni iniziali:
            
        $$ A = \left[\begin{matrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix} \right] = 
           \left[\begin{matrix}-0.15 & 2.10 \\ -2.10 & -0.10 \end{matrix} \right] $$

            $$ \begin{equation}
            \begin{cases}
                x(0)=2.5
                \\
                y(0)=0.5
            \end{cases}
            \end{equation} $$


            in modo da poter creare il dataset della coppia delle <i>vere</i> serie temporali:
            il tempo viene discretizzato in modo arbitrario da $0$ a $4$ con un numero di $51$ valori discreti,
            mentre le serie temporali $x(t)$ e $y(t)$ sono costruite risolvendo il sistema di equazioni differenziali
            ottenuto usando la matrice A dei parametri specifici nel sistema parametrico di cui sopra, insieme alle condizioni iniziali scelte arbitrariamente.<br />
            Una volta preparato il dataset (che nel mondo reale pu&ograve; essere ottenuto eseguendo delle vere misurazioni)
            ci si dimentichi dei valori dei parametri (che quindi diventeranno sconosciuti) e ci si ponga un duplice problema:
            <ul>
                <li>come addestrare una rete neurale che incorpori la legge nota per imparare il sistema a parametri sconosciuti al fine di approssimare al meglio le serie temporali iniziali?</li>
                <li>come utilizzare tale rete neurale una volta addestrata per predire valori futuri (il forecast, appunto)?</li>
            </ul>
        </p>

 		<h3>L&apos;implementazione della soluzione</h3>
        <p>
            La legge matematica del sistema di cui sopra in Julia si implementa cos&igrave;:
            <pre><code class="julia">math_law(u) = sin.(2. * u) + cos.(2. * u)</code></pre>
            Il dataset di input, costruito risolvendo il sistema di equazioni differenziali <i>vere</i>, si crea eseguendo questo codice:<br />
            <br />
            <pre><code class="julia">function true_ode(du,u,p,t)
    true_A = [-0.15 2.10; -2.10 -0.10]
    du .= ((math_law(u))'true_A)'
end

tbegin = 0.0
tend = 4.0
datasize = 51
t = range(tbegin,tend,length=datasize)
u0 = [2.5; 0.5]
tspan = (tbegin,tend)
trange = range(tbegin,tend,length=datasize)
prob = ODEProblem(true_ode, u0, tspan)
dataset_ts = Array(solve(prob, Tsit5(), saveat=trange))</code></pre>
            <b>Nota</b>: tale codice contiene delle dichiarazioni di variabili che saranno utilizzate nei frammenti di codice successivi.<br />
            <br />

            La rete neurale &egrave; costituita da uno strato che rappresenta la legge matematica pi&ugrave;
            due layer di tipo <em>Dense</em> (detti anche <em>Full Connected</em>) con 2 nodi in ingresso, 2 in uscita e 50 nello strato intermedio
            e la tangente iperbolica (<em>tanh</em>) quale funzione di attivazione; questo il codice:<br />
            <br />
            <pre><code class="julia">dudt = Chain(u -> math_law(u),
             Dense(2, 50, tanh),
             Dense(50, 2))

reltol = 1e-7
abstol = 1e-9
n_ode = NeuralODE(dudt, tspan, Tsit5(), saveat=trange, reltol=reltol,abstol=abstol)
ps = Flux.params(n_ode.p)</code></pre>
            dove <code>ps</code> referenzia i parametri addestrabili della rete.<br />
            <br />
            <b>Nota</b>: lo scopo nella rete <u>NON</u> &egrave; quello di risolvere il sistema di equazioni differenziali
            ma quello di imparare ad approssimare il sistema delle <i>vere</i> ODE (che in questa demo &egrave; servito per generare il dataset).<br />
            <br />

            Le impostazioni dell&apos;addrestramento sono:<br />
            <ul>
                <li>Ottimizzatore: ADAM</li>
                <li>Learning rate: $0.01$</li>
                <li>Numero di epoche: $400$</li>
                <li>Funzione di loss: somma dei quadrati delle differenze</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="julia">function loss_n_ode()
  pred = n_ode(u0)
  loss = sum(abs2, dataset_ts1,: .- pred1,:) +
         sum(abs2, dataset_ts2,: .- pred2,:)
  loss
end

n_epochs = 400
learning_rate = 0.01
data = Iterators.repeated((), n_epochs)
opt = ADAM(learning_rate)

cb = function ()
  loss = loss_n_ode()
  println("Loss: ", loss)
end

println();
cb()

Flux.train!(loss_n_ode, ps, data, opt, cb=cb)]</code></pre>
            <br />

            I setting per il forecast sono:<br />
            <ul>
                <li>Tempo di inizio: coincidente con il tempo di fine della serie temporale di input, quindi $4$.</li>
                <li>Tempo di fine: tempo di inizio del forecast pi&ugrave; una quantit&egrave; di tempo a piacere, diciamo 5, quindi $4+5$, cio&egrave; $9$.</li>
                <li>Numero di valori temporali: 351 (quindi molti pi&ugrave punti e molto pi&ugrave; fitti rispetto alle serie temporali di input).</li>
                <li>Condizione iniziale: gli ultimi valori delle serie temporali di input.</li>
            </ul>
            In codice:<br />
            <br />
            <pre><code class="julia">tbegin_forecast = tend
tend_forecast = tbegin_forecast + 5.0
tspan_forecast = (tbegin_forecast, tend_forecast)
datasize_forecast = 351
trange_forecast = range(tspan_forecast[1], tspan_forecast[2], length=datasize_forecast)
u0_forecast = [dataset_ts[1,datasize], dataset_ts[2,datasize]]</code></pre>
            <br />
            Per calcolare il forecast si esegua:<br />
            <pre><code class="julia">tbegin_forecast = tend
tend_forecast = tbegin_forecast + 5.0
tspan_forecast = (tbegin_forecast, tend_forecast)
datasize_forecast = 351
trange_forecast = range(tspan_forecast[1], tspan_forecast[2], length=datasize_forecast)
u0_forecast = [dataset_ts[1,datasize], dataset_ts[2,datasize]]

n_ode_forecast = NeuralODE(
  n_ode.model, tspan_forecast;
  p=n_ode.p, saveat=trange_forecast, reltol=reltol, abstol=abstol)
forecast = n_ode_forecast(u0_forecast)</code></pre>
            <br />

            Il codice completo contiene anche degli statement per visualizzare il grafico di sei curve:
            <ul>
                <li>Le due serie temporali originali in input.</li>
                <li>Le due serie temporali cos&igrave; come le approssima la rete neurale dopo l&apos;addestramento.</li>
                <li>La proiezione nel futuro delle due serie temporali (forecast).</li>
            </ul>
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.<br/>
			<br />
        </p>

        </p>
        <div class="betweentextlines"><img src="../../posts/neural-networks/experiments-with-neural-odes-in-julia/nn-expnodesjl-exp-2-result.png" /></div>
        <div class="photocaption">Visualizzazione delle serie in input originali, delle serie imparate e loro proiezione futura</div>
        <br />

        <p>
            Dal grafico si evince chiaramente che la rete neurale ha imparato bene le propriet&agrave; di oscillazione e smorzatura delle due serie
            e riesce a proiettarle nel fututo.<br />
            <br />
            Si noti soprattutto il concetto di <em>profonditt&agrave; continua</em> tipico delle NeuralODEs, infatti mentre nelle classiche reti neurali
            il passo di discretizzazione delle serie temporali di input &egrave; lo stesso nel forecast, con le NeuralODEs invece l&apos;array dei tempi
            con cui calcolare la predizione pu&ograve; essere arbitrariamente fitto, proprio perch&eacute; il risolutore di equazioni differenziali
            dentro la rete neurale, per sua propria natura, lavora nel continuo e trasmette questa propriet&agrave; a tutta la rete.<br />
            <br /> 
            Qui il link al codice completo su <a target="_blank" href="https://github.com/ettoremessina/differential-equations/blob/main/ODEs/neural-odes-demos/julia/DiffEqFlux/train-nn-sys-of-odes-for-forecast.jl">GitHub</a>.<br />
            <br />
        </p>

        <h2>Citazioni</h2>
        <p>
            <pre><code class="text">@articleDBLP:journals/corr/abs-1806-07366,
  author    = {Tian Qi Chen and
               Yulia Rubanova and
               Jesse Bettencourt and
               David Duvenaud},
  title     = {Neural Ordinary Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1806.07366},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.07366},
  archivePrefix = {arXiv},
  eprint    = {1806.07366},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-07366.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
</code></pre>
            <pre><code class="text">@articleDBLP:journals/corr/abs-1902-02376,
  author    = {Christopher Rackauckas and
               Mike Innes and
               Yingbo Ma and
               Jesse Bettencourt and
               Lyndon White and
               Vaibhav Dixit},
  title     = {DiffEqFlux.jl - {A} Julia Library for Neural Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1902.02376},
  year      = {2019},
  url       = {https://arxiv.org/abs/1902.02376},
  archivePrefix = {arXiv},
  eprint    = {1902.02376},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-02376},
  bibsource = {dblp computer science bibliography, https://dblp.org}
</code></pre>
        </p>

        <h2 id="downloadcode">Download del codice completo</h2>
		<p>
			Il codice completo &egrave; disponibile su <a target="_blank" href="https://github.com/ettoremessina/differential-equations/tree/main/ODEs/neural-odes-demos/julia/DiffEqFlux/">GitHub</a>.
			<br/>
			
			Questo materiale &egrave; distribuito su licenza MIT; sentiti libero di usare, condividere, &quot;forkare&quot; e adattare tale materiale come credi.
			<br/>
			Sentiti anche libero di pubblicare pull-request e bug-report su questo repository di GitHub oppure di contattarmi sui miei canali social disponibili nell&apos;angolo in alto a destra di questa pagina. 
			<br/>

		</p>

	</section>

						</div>
				</div>
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<div class="align-center"><img src="../../images/cm-logo-small.png" alt="Mentalit&agrave;&nbsp;Computazionale"/></div>
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="../../it/">Home</a></li>
										<li>
											<span class="opener">Reti&nbsp;Neurali</span>
											<ul>
												<li><a href="../../it/reti-neurali/">INDICE</a></li>
												<li><a href="../../it/reti-neurali/equazioni-differenziali-e-reti-neurali.html">Equazioni Differenziali e Reti Neurali</a></li>
												<li><a href="../../it/reti-neurali/forecast-di-una-serie-temporale-univariata-equispaziata-con-tensorflow.html">Forecast di una serie temporale univariata ed equispaziata con TensorFlow</a></li>
												<li><a href="../../it/reti-neurali/approssimazione-con-percettroni-multistrato-altamente-configurabili.html">Approssimazione con percettroni multistrato altamente configurabili</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Computazione&nbsp;Quantistica</span>
											<ul>
												<li><a href="../../it/computazione-quantistica/">INDICE</a></li>
												<li><a href="../../it/computazione-quantistica/operatori-not-cnot.html">Porte quantistiche NOT e C-NOT</a></li>
												<li><a href="../../it/computazione-quantistica/generazione-numero-casuale.html">Generazione di un numero casuale</a></li>
												<li><a href="../../it/computazione-quantistica/porte-hadamard-in-cascata.html">Porte Hadamard in cascata</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Machine&nbsp;Learning</span>
											<ul>
												<li><a href="../../it/machine-learning/">INDICE</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html">Approssimazione di funzioni tramite un regressore XGBoost configurabile</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-svr-configurabile.html">Approssimazione di funzioni tramite un Support Vector Regressor configurabile</a></li>
												<li><a href="../../it/machine-learning/regressione-polinomiale-con-accord-net.html">Regressione polinomiale con Accord.NET</a></li>
												<li><a href="../../it/machine-learning/regressione-smo-con-kernel-puk-in-weka.html">Regressione con SMO per SVM con kernel PUK in Weka</a></li>
												<li><a href="../../it/machine-learning/forecast-smo-con-kernel-polinomiale-in-weka.html">Forecast con SMO per SVM con kernel polinomiale in Weka</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Matematica</span>
											<ul>
												<li><a href="../../it/matematica/">INDICE</a></li>
												<li><a href="../../it/matematica/esperimenti-con-sympy-per-risolvere-odes-ordine-1.html">Esperimenti con SymPy per risolvere equazioni differenziali ordinarie del 1&deg; ordine</a></li>
												<li><a href="../../it/matematica/metodo-soluzione-dde-primo-ordine-usando-funzione-w-lambert.html">Un metodo di soluzione di una equazione differenziale con ritardo del primo ordine utilizzando la funzione W di Lambert</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Dataset</span>
											<ul>
												<li><a href="../../it/datasets/">INDICE</a></li>
												<li><a href="../../it/datasets/functions-dataset.html">Collezione di dataset &apos;Functions&apos;</a></li>
												<li><a href="../../it/datasets/time-series-dataset.html">Collezione di dataset &apos;Time&nbsp;Series&apos;</a></li>
												<li><a href="../../it/datasets/synthetic-words-dataset.html">Dataset &apos;Synthetic Words&apos;</a></li>
											</ul>
										</li>
										<li><a href="../../it/info.html">Info</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
                                    <div class="side-topics">
                                        <header class="align-center">
                                            <h2><a href="../../it/reti-neurali/">Reti&nbsp;Neurali</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/reti-neurali/" class="image"><span class="icon solid fa-sitemap"/></a>
                                        </article>
                                        <header class="align-center">
                                            <h2><a href="../../it/computazione-quantistica/">Computazione&nbsp;Quantistica</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/computazione-quantistica/" class="image"><span class="icon solid fa-atom"/></a>
                                        </article>
                                    </div>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">
										Grafica basata sul template &apos;Editorial&apos; (con personalizzazione) scaricato da <a href="https://html5up.net" target="_blank">HTML5 UP</a>.
										<br/>
										Clicka sui link per vedere i file <a href="../../html5up-license/LICENSE.txt" target="_blank">LICENSE.txt</a> e <a href="../../html5up-license/README.txt" target="_blank">README.txt</a> del template &apos;Editorial&apos; di HTML5 UP.
										<br>
										<br>
										&copy; <a href="../../it/info.html">Ettore Messina</a>. 
									</p>
								</footer>
						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

			<style>
				a.cc-link
				{
			    	border-bottom: none;
				}
				a.cc-link:hover
				{
					color: white !important;
				}
			</style>
	</body>
</html>


<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="it">
	<head>
		<meta http-equiv="content-language" content="it">
		<meta name="author" content="Ettore Messina">

		<style>
			#cookiescript_checkbox_input {
				-moz-appearance: checkbox;
				-webkit-appearance: checkbox;
				-ms-appearance: checkbox;
				appearance: checkbox;
				opacity: 1.0;
			}
			#cookiescript_checkbox_text {
				color: white;
			}
			#cookiescript_description a:hover {
				color: yellow !important;
			}
		</style>
		<script type="text/javascript" charset="UTF-8" src="https://cookie-script.com/s/19e1626ea9f21a6fcc285b559b5957e6.js"></script>


		<title>Pruning di reti neurali con TensorFlow</title>
		<meta name="description" content="Il pruning dei pesi di una rete neurale pone a zero i pesi insignificanti del modello in fase di addestramento con il fine di ottenere un certo livello di sparsit&agrave; in modo tale da rendere il modello pi&ugrave; facilmente comprimibile." >
		<meta name="keywords" content="pruning, potatura, sparsit&agrave;, pesi insignificanti, model optimization, TensorFlow Optimization, PolynomialDecay, ConstantSparsity, prune_low_magnitude, UpdatePruningStep, strip_pruning" >
		<link rel="canonical" href="https://computationalmindset.com/it/reti-neurali/pruning-di-reti-neurali-con-tensorflow.html" />
		<link rel="alternate" hreflang="en" href="https://computationalmindset.com/en/neural-networks/pruning-of-neural-networks-with-tensorflow.html" />
		<link rel="alternate" hreflang="it" href="https://computationalmindset.com/it/reti-neurali/pruning-di-reti-neurali-con-tensorflow.html" />
		
    <!-- SCHEMA.ORG JSON-LD WEBSITE -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "name": "Computational Mindset",
        "url": "https://computationalmindset.com/",
        "sameAs": ["https://www.facebook.com/ComputationalMindset/", "https://github.com/ettoremessina/"],
        "author":
        {
          "@type": "Person",
          "name": "Ettore Messina",
          "image": "https://computationalmindset.com/images/ettore-messina.jpg",
          "gender": "Male",          
          "sameAs": ["https://www.facebook.com/ettore.messina.73/", "https://www.instagram.com/etmessina/", "https://twitter.com/ettoremessina/", "https://twitter.com/Computational_M/", "https://github.com/ettoremessina/", "https://medium.com/@ettoremessina/", "https://www.linkedin.com/in/ettoremessina", "https://www.linkedin.com/company/computational-mindset", "https://ettoremessina.tech/"]
        }
    }
    </script>

		
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement":
        [
		
          {
            "@type": "ListItem",
            "position": 1,
            "item":
            {
                "@id": "https://computationalmindset.com/it/",
                "name": "Mentalit&agrave; Computazionale"
            }
          },
		
          {
            "@type": "ListItem",
            "position": 2,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/",
              "name": "Reti Neurali"
            }
          },

          {
            "@type": "ListItem",
            "position": 3,
            "item":
            {
              "@id": "https://computationalmindset.com/it/reti-neurali/pruning-di-reti-neurali-con-tensorflow.html",
              "name": "Pruning di reti neurali con TensorFlow"
            }
		  }
		
        ]
    }
    </script>		
		<style>
			.valign-middle
			{
				vertical-align: middle;
			}
			.size-of-symbols
			{
				font-size: 20px;
			}
		</style>
		

		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/railscasts.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
		
		<script>hljs.initHighlightingOnLoad();</script>
		<style>
			pre > code 
			{
				font-size: 1.2em;
			}
		</style>

		
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!-- OPEN GRAPH PROTOCOL --------------------------------------------------------------------->
		<meta property="og:url" content="https://computationalmindset.com/it/reti-neurali/pruning-di-reti-neurali-con-tensorflow.html" />
		<meta property="og:type" content="article" />
		<meta property="og:title" content="" />
		<meta property="og:description" content="" />
		<meta property="og:image" content="/social-previews/thumbnail21.jpg" />
		<!-- OPEN GRAPH PROTOCOL --------------------------------------------------------------------->


		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
		<link rel="icon" href="../../favicon.ico" type="image/x-icon" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../it/info.html" class="logo"><strong>Mentalit&agrave;&nbsp;Computazionale</strong> di&nbsp;Ettore&nbsp;Messina</a>
									<div style="text-align:right">
										<a class="logo" href="../../en/">en</a>
										&nbsp;&nbsp;&nbsp;
										<a class="logo" href="../../it/">it</a>
									</div>
									<ul class="icons">
										<li><a href="https://github.com/ettoremessina/" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.linkedin.com/company/computational-mindset" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://twitter.com/Computational_M/" class="icon brands fa-twitter" target="_blank"><span class="label">Twitter</span></a></li>
										<li><a href="https://www.facebook.com/ComputationalMindset/" class="icon brands fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.youtube.com/c/ComputationalMindset" class="icon brands fa-youtube" target="_blank"><span class="label">YouTube</span></a></li>
										<li><a href="https://linktr.ee/ComputationalMindset/" class="fas fa-link" style="color: grey;" target="_blank"><span class="label"></span></a></li>
										<li><a href="https://ettoremessina.tech/" class="icon brands fa-wordpress" target="_blank"><span class="label">WordPress</span></a></li>
									</ul>
								</header>

<!-- Content -->
	<section>
		<header class="main">
			<h1>Pruning di reti neurali con TensorFlow</h1>
		</header>
		<p>
			Lo scopo del <em>pruning</em> (ovverosia della <em>potatura</em>) dei pesi basato sulla magnitudine &egrave; quello di azzerare gradualmente,
			durante la fase di addestramento, i pesi del modello meno significativi ottenendo cos&igrave; un certo grado di <em>sparsit&agrave;</em> 
			nelle matrici dei pesi (sia kernel che bias) del modello.<br />
			Per sparsit&agrave; di una matrice si intente la presenza di elementi uguali a zero nella matrice stessa: pi&ugrave; sono presenti elementi uguali a zero
			pi&ugrave; la matrice ha un grado di sparsit&agrave; maggiore; una matrice sparsa porta vantaggi in termini di occupazione di memoria e computazionali.
			Per quanto riguarda la memoria pu&ograve; essere pi&ugrave; facilmente compressa grazie alla presenza di elmenti ridondanti (gli zeri, appunto)
			ed &egrave; questo in caso trattato in questo post; in generale le matrici sparse possono essere anche memorizzate in modo differente dai tradizionali
			array NxM, memorizzando ad esempio in liste i valori diversi da zero con i relativi indici associati, ma non &egrave; questo il caso trattato in questo post.<br />
			Per quanto riguarda gli aspetti computazionali ci potrebbero essere degli spazi di miglioramento in quanto le moltiplicazioni con gli elementi uguali a zero
			potrebbero essere saltate, ma anche questo caso non &egrave; trattato in questo post, in quando qui il focus &egrave; realizzare la sparsit&agrave;
			delle matrici di pesi al fine di ottenere compressione maggiore a fronte di una limitata perdita di qualit&agrave; dell&apos;inferenza (la predizione) del modello stesso.<br />
			<br />
			Il post presenta una soluzione di pruning di pesi basato sulla magnitudine implementata tramite la libreria
			<em><a href="https://www.tensorflow.org/model_optimization" target="_blank">TensorFlow Model Optimization</a></em>
			e mostra tre esempi di pruning applicati a tre differenti tipologie di reti: una rete full-connected (un MLP che realizza una regressione), una rete long-short-term-memory
			(una rete LSTM che realizza un forecast di una serie temporale) e una rete convoluzionaria (una rete CNN che realizza un classificatore di immagini).<br />
			Il codice descritto da questo post richiede la versione 3 di Python e utilizza la tecnologia TensorFlow 2.x (per CPU o GPU) con Keras (che &egrave; gi&agrave; integrato dentro TensorFlow 2);
			richiede oltre alla gi&agrave; citata TensorFlow Model Optimization altre librerie quali NumPy, SkLearn, Pandas, MatPlotLib e TensorFlow Datasets.<br/>
			<br />
			Per ottenere il codice si veda il paragrafo <a href="#downloadcode">Download del codice completo</a> in fondo a questo post.<br/>
		</p>

        <h2>Il codice della soluzione</h2>
		<p>
			Il cuore della soluzione proposta &egrave; il file <code><a href="https://github.com/ettoremessina/nn-optimization/blob/main/pruning/demos/tensorflow/support/trim_insignificant_weights.py" target="_blank">trim_insignificant_weights.py</a></code>
			che implementa due classi e alcune funzioni.<br />
			<br />
			Le classi sono:
			<ul>
				<li>
					<code>AttemptConfig</code>:
					che costituisce l&apos;elemento di una griglia di ricerca (implementata dagli esempi mostrati successivamente) che procede per tentativi;
					tale classe implementa due propriet&agrave;: il nome del tentativo e l&apos;oggetto di TensorFlow Model Optimization che implementa la politica di pruning.
					Attualmente le politiche supportate sono: <code><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay" target="_blank">PolynomialDecay</a></code>
					e <code><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/ConstantSparsity" target="_blank">ConstantSparsity</a></code>.
				</li>
				<li>
					<code>AttemptInfo</code>:
					che implementa una serie di propriet&agrave; per memorizzare varie informazioni relative a un modello addestrato. Precisamente: 
					il numero totale di pesi, il numero dei pesi uguali a zero (e per differenza quelli diversi da zero),
					la size del modello originale salvato in .h5, la size del file .h5 zippato, la size del file .tflite, la size del file .tflite zippato con i relativi coefficienti di compressione 
					e inoltre altre informazioni riguardanti l&apos;inferenza, quali i valori di test predetti e l&apos;errore tra i valori predetti e quelli attesi.
				</li>
			</ul>
			Le funzioni sono:<br />
			<br />
			<ul>
				<li>
					<code>print_attempt_infos</code>:
					prende in ingresso una lista di oggetti di tipo <code>AttemptInfo</code>, ottenuta dall&apos;esecuzione completa della grigia di ricerca,
					e scrive sullo standard output, in modo user-friendly, il contenuto informativo dei vari oggetti <code>AttemptInfo</code> presenti nella lista.
				</li>
				<li>
					<code>inspect_weigths</code>:
					prende in ingresso un modello keras, ne ispeziona i pesi (sia kernel che bias) e scrive sullo standard output il numero dei pesi per ciascun layer del modello
					indicando quanti di essi sono uguali a zero e quanti diversi da zero.
				</li>
				<li>
					<code>retrieve_size_of_model</code>:
					prende in input un modello keras e restituisce la size del file .h5 che si ottiene salvando il modello e la size dello stesso file zippato.
				</li>
				<li>
					<code>retrieve_size_of_lite_model</code>:
					prende in input un modello keras e restituisce la size del file .tflite che si ottiene convertendo e salvando il modello per TensorFlow Lite;
					inoltre, come sopra, restituisce la size dello stesso file zippato.
				</li>
				<li>
					<code>build_pruning_model</code>:
					prende in input un modello keras originale (che non ha subito alcun processo di pruning) e restituisce un wrapper del modello applicando il metodo
					<code><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude" target="_blank">prune_low_magnitude</a></code> 
					per preparare il modello a subire un processo di pruning durante la fase di training.
				</li>
				<li>
					<code>retrieve_callbacks_for_pruning</code>:
					restituisce la callback <code><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/UpdatePruningStep" target="_blank">UpdatePruningStep</a></code> necessaria alla fase di addestramento con pruning.
				</li>
				<li>
					<code>extract_pruned_model</code>:
					prende in input un wrapper per il pruning di un modello e rimuove il wrapper, tramite <code><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning" target="_blank">strip_pruning</a></code>
					e restituisce il modello sottostante che risulta pronto per l&apos;inferenza.
				</li>
			</ul>
		</p>

        <h2>Gli esempi</h2>
		<p>
			Gli esempi mostrati in questo post seguono tutti un medesimo schema: il codice di esempio prepara un dataset (che &egrave; generato sinteticamente negli esempi #1 e #2,
			mentre nell&apos;esempio #3 &egrave; utilizzato un dataset di <a href="https://www.tensorflow.org/datasets/" target="_blank">TensorFlow Datasets</a>,
			che viene diviso in due pezzi: uno per il training e l&apos;altro per il test (a volte vi &egrave; un terzo pezzo per la validazione);
			quindi viene costruito un modello di rete neurale ed eseguito un processo di addestramento.<br /> 
			Questo modello &egrave; denominato <em>modello originale</em> e le informazioni su di esso vengono inserite in un&apos;istanza della classe <code>AttemptInfo</code>.<br />
			A questo punto viene creata la griglia di ricerca, ovverosia una lista di instanze delle classi <code>PolynomialDecay</code> e <code>ConstantSparsity</code>
			inizializzate in modo differente; ogni configurazione &egrave; memorizzata in una istanza di <code>AttemptConfig</code>.<br />
			Il codice dell&apos;esempio cicla sulla griglia di ricerca e per ogni <code>AttemptConfig</code> crea un wrapper per il pruning del modello originale
			chiamando la funzione <code>build_pruning_model</code> e addestra questo modello, usando gli stessi iper-parametri dell&apos;addestramento del modello originale
			ma con una callback in pi&ugrave; ottenuta chiamando <code>retrieve_callbacks_for_pruning</code>.<br />
			Durante l&apos;addestramento, il modello subisce un processo di pruning; terminato addestramento, viene chiamata la funzione <code>extract_pruned_model</code>
			per rimuovere il wrapper e ottenere il modello sottostante su cui si effettua l&apos;inferenza dei dati di test
			e infine si memorizzano in una nuova instanza di <code>AttemptInfo</code> tutte le informazioni su tale modello, in particolare la size del file .h5 zippato
			e la size del file .tflite zippato e l&apos;errore calcolato confrontanto la predizione sui dati di test e i veri valori di test.<br />
			Le varie instanze di <code>AttemptInfo</code> sono raccolte in una lista e alla fine dello script di esempio sono visualizzate le informazioni
			dei vari tentativi per consentire di confrontare il fattore di compressione ottenuto per ogni tentativo a fronte di quanta perdita di qualit&agrave; del modello.<br />
			Per gli esempi #1 e #2 &egrave; anche mostrato, per ogni tentativo, un grafico cartesiano con due curve: in verde il dataset di test, in rosso la predizione del tentativo corrente
			e questo consente di avere una visione della perdita di qualit&agrave; al crescere dell&apos;attivit&agrave; di pruning.
		</p>

        <h3>Esempio #1: rete neurale full-connected</h3>
		<p>
			Il codice di questo esempio &egrave; il file <code><a href="https://github.com/ettoremessina/nn-optimization/blob/main/pruning/demos/tensorflow/example1.py" target="_blank">example1.py</a></code>.<br />
			Il dataset utilizzato da questo esempio &egrave; un dataset sintetico cos&igrave; generato:
			<pre><code class="python">fx_gen_ds = lambda x: x**2 #generating function of the dataset
x_dataset = np.arange(-2., 2, 0.005, dtype=float)
y_dataset = fx_gen_ds(x_dataset)</code></pre>
			che &egrave; banalmente una curva parabolica la cui equazione &egrave; $y=x^2$ con $x \in [-2, 2]$.<br />
			Per eseguire questo scripy Python lanciare il seguente comando:<br />
			<br/>
			<pre><code class="shell">$ python example1.py</code></pre>
			Nell&apos;output ottenuto, proprio all&apos;inizio, si osserva la struttura del modello, che &egrave; una normalissima rete full-connected,
			ovverosia un MLP (Multi Layer Perceptron) implementato tramite il layer <code>Dense</code>, con un totale di 4289 pesi addestrabili.<br />
			Qui la struttura della rete:<br />
			<br />
			<pre><code class="shell">Model: "mlp_regression_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
dense (Dense)                (None, 32)                64        
_________________________________________________________________
dense_1 (Dense)              (None, 64)                2112      
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080      
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 33        
=================================================================
Total params: 4,289
Trainable params: 4,289
Non-trainable params: 0
_________________________________________________________________</code></pre>
			Segue l&apos;addestramento del modello originale che consiste in 100 epoche con batch size di 80 elementi, con ottimizzatore <code>Adams</code>
			e <code>MeanSquaredError</code> quale funzione di loss. Le propriet&agrave; di questo modello (che chiamiamo modello originale in quanto non sottoposto a pruning)
			sono visibili nell&apos;output del programma:<br />
			<br/>
			<pre><code class="shell">Model: original (unpruned)
  Total number of weights: 4289
  Total number of non-zero weights: 4280
  Total number of zero weights: 9
  Unzipped h5 size: 37272 bytes
  Zipped h5 size: 18155 bytes (compression factor: 51.29%)
  Unzipped tflite size: 7232 bytes
  Zipped tflite size: 5781 bytes (compression factor: 20.06%)
  Error (loss) value: 1.379212E-04</code></pre>
			da cui si evince che l&apos;errore calcolato sul dataset di test &egrave; molto basso ($1.379212 \cdot 10^{-4}$), pochissimi sono i pesi uguali a zero (meno di 10, praticamente nessuno) come ci si aspettava
			in quanto quello modello non &egrave; stato sottoposto a pruning, il fattore di compressione sul file .h5 &egrave; del $51.29 \%$
			mentre quello sul file .tflite &egrave; del $20.06 \%$.<br />
			L&apos;immagine seguente mostra a colpo d&apos;occhio che la qualit&agrave; dell&apos;inferenza &egrave; molto buona.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example1-attempt-org.png" alt="Dataset di test e inferenza prodotta con il modello originale"/></div>
			<div class="photocaption">Dataset di test (in verde) e inferenza (in rosso) prodotta con il modello originale (che non ha subito alcun pruning).</div>
			<br/>
			Segue quindi l&apos;esecuzione della griglia di ricerca che esegue 11 tentativi di applicazione di pruning, 6 con <code>PolynomialDecay</code> variamente inizializzati
			e 5 con <code>ConstantSparsity</code> variamente inizializzati. A titolo di campione, si mostra il risultato di uno degli 11 tentativi, precisamente <em>const sparsity 0.5</em>
			(comunque i risultati di tutti i tentativi sono disponibili sullo standard output):<br />
			<br/>
			<pre><code class="shell">Model: const sparsity 0.5
  Total number of weights: 4289
  Total number of non-zero weights: 1775
  Total number of zero weights: 2514
  Unzipped h5 size: 37272 bytes
  Zipped h5 size: 10832 bytes (compression factor: 70.94%)
  Unzipped tflite size: 7232 bytes
  Zipped tflite size: 2953 bytes (compression factor: 59.17%)
  Error (loss) value: 3.109528E-04</code></pre>
			da cui si evince che l&apos;errore calcolato sul dataset di test resta molto basso ($3.109528 \cdot 10^{-4}$) anche se un po&apos; pi&ugrave; alto del modello originale come ci si aspettava
			in quanto il pruning riduce la qualit&agrave; del modello; i pesi uguali a zero sono 2514 su 4289 (quindi pi&ugrave; della met&agrave;), il fattore di compressione sul file .h5 &egrave; del $70.94 \%$
			mentre quello sul file .tflite &egrave; del $59.17 \%$.<br />
			L&apos;immagine seguente mostra a colpo d&apos;occhio che la qualit&agrave; dell&apos;inferenza &egrave; piuttosto buona.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example1-attempt-pruned.png" alt="Dataset di test e inferenza prodotta con il modello const sparsity 0.5" /></div>
			<div class="photocaption">Dataset di test (in verde) e inferenza (in rosso) prodotta con il modello <em>const sparsity 0.5</em>.</div>
			<br/>
			Terminati tutti i tentativi, lo script di esempio mostra il recap di tutti i tentativi; il primo elemento del recap &egrave; relativo al modello originale.<br />
			<br/>
			<pre><code class="shell">*** Final recap ***
Attempt name              Size h5 (Comp. %)     Error (loss)
original (unpruned)         18155 ( 51.29%)     1.379212e-04
poly decay 10/50            11958 ( 67.92%)     1.981978e-03
poly decay 20/50            11928 ( 68.00%)     9.621178e-05
poly decay 30/60            10544 ( 71.71%)     2.460897e-04
poly decay 30/70             9039 ( 75.75%)     2.291273e-03
poly decay 40/50            12254 ( 67.12%)     8.707970e-05
poly decay 10/90             5782 ( 84.49%)     3.172360e-02
const sparsity 0.1          10858 ( 70.87%)     5.406314e-04
const sparsity 0.4          10856 ( 70.87%)     4.125351e-04
const sparsity 0.5          10832 ( 70.94%)     3.109528e-04
const sparsity 0.6          10476 ( 71.89%)     2.269561e-04
const sparsity 0.9           5792 ( 84.46%)     9.419761e-04</code></pre>
			da cui si evince che in linea di principio, all&apos;aumentare del fattore di compressione aumenta l&apos;errore calcolato sul dataset di test
			e quindi diminuisce la qualit&agrave; dell&apos;inferenza.<br />
			A titolo di campione l&apos;immagine seguente mostra un modello che ha subito un pesante pruning e conseguentemente la qualit&agrave; dell&apos;inferenza
			&egrave; notevolmente peggiorata rispetto al modello precedentemente mostrato.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example1-attempt-weak.png" alt="Dataset di test e inferenza prodotta con il modello poly decay 10/90" /></div>
			<div class="photocaption">Dataset di test (in verde) e inferenza (in rosso) prodotta con il modello <em>poly decay 10/90</em>.</div>
			<br/>
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.<br/>
		</p>

        <h3>Esempio #2: rete neurale long-short-term-memory</h3>
		<p>
			Il codice di questo esempio &egrave; il file <code><a href="https://github.com/ettoremessina/nn-optimization/blob/main/pruning/demos/tensorflow/example2.py" target="_blank">example2.py</a></code>.<br />
			Il dataset utilizzato da questo esempio &egrave; una serie temporale sintetica cos&igrave; generata:
			<pre><code class="python">ft_gen_ts = lambda t: 2.0 * np.sin(t/10.0) #generating function of the time series
t_train = np.arange(0, 200, 0.5, dtype=float)
y_train_timeseries = ft_gen_ts(t_train)
t_test = np.arange(200, 400, 0.5, dtype=float)
y_test_timeseries = ft_gen_ts(t_test)</code></pre>
			che &egrave; banalmente una sinusoide la cui equazione &egrave; $y=2 \sin \frac{t}{10}$ con $t \in [0, 200]$.<br />
			Per eseguire questo scripy Python lanciare il seguente comando:<br />
			<br/>
			<pre><code class="shell">$ python example2.py</code></pre>
			Nell&apos;output ottenuto, proprio all&apos;inizio, si osserva la struttura del modello, che &egrave; una rete con un layer LSTM seguito da un layer Dense,
			adatta a calcolare un forecast; ha con un totale di 26321 pesi addestrabili.<br />
			Qui la struttura della rete:<br />
			<br />
			<pre><code class="shell">Model: "long_short_term_memory_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 6, 1)]            0         
_________________________________________________________________
lstm (LSTM)                  (None, 80)                26240     
_________________________________________________________________
dense (Dense)                (None, 1)                 81        
=================================================================
Total params: 26,321
Trainable params: 26,321
Non-trainable params: 0
_________________________________________________________________</code></pre>
			Segue l&apos;addestramento del modello originale che consiste in 80 epoche con finestra temporale pari a 6 elementi e batch size di 50 elementi,
			con ottimizzatore <code>Adams</code> e <code>MeanSquaredError</code> quale funzione di loss.
			Le propriet&agrave; di questo modello (che chiamiamo modello originale in quanto non sottoposto a pruning)
			sono visibili nell&apos;output del programma:<br />
			<br/>
			<pre><code class="shell">Model: original (unpruned)
  Total number of weights: 26321
  Total number of non-zero weights: 26321
  Total number of zero weights: 0
  Unzipped h5 size: 122096 bytes
  Zipped h5 size: 99952 bytes (compression factor: 18.14%)
  Unzipped tflite size: 42480 bytes
  Zipped tflite size: 29678 bytes (compression factor: 30.14%)
  Error (loss) value: 1.289916E-01</code></pre>
			da cui si evince che l&apos;errore calcolato sul forecast di test &egrave; relativamente basso ($1.289916 \cdot 10^{-1}$), nessun peso uguale a zero come ci si aspettava
			in quanto quello modello non &egrave; stato sottoposto a pruning, il fattore di compressione sul file .h5 &egrave; del $18.14 \%$
			mentre quello sul file .tflite &egrave; del $30.14 \%$.<br />
			L&apos;immagine seguente mostra a colpo d&apos;occhio che la qualit&agrave; del forecast &egrave; buona.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example2-attempt-org.png" alt="Dataset di test e forecast prodotto con il modello originale" /></div>
			<div class="photocaption">Dataset di test (in verde) e forecast (in rosso) prodotto con il modello originale (che non ha subito alcun pruning).</div>
			<br/>
			Segue quindi l&apos;esecuzione della griglia di ricerca che esegue 11 tentativi di applicazione di pruning, 6 con <code>PolynomialDecay</code> variamente inizializzati
			e 5 con <code>ConstantSparsity</code> variamente inizializzati. A titolo di campione, si mostra il risultato di uno degli 11 tentativi, precisamente <em>poly decay 40/50</em>
			(comunque i risultati di tutti i tentativi sono disponibili sullo standard output):<br />
			<br/>
			<pre><code class="shell">Model: poly decay 40/50
  Total number of weights: 26321
  Total number of non-zero weights: 13322
  Total number of zero weights: 12999
  Unzipped h5 size: 122096 bytes
  Zipped h5 size: 63464 bytes (compression factor: 48.02%)
  Unzipped tflite size: 42480 bytes
  Zipped tflite size: 21277 bytes (compression factor: 49.91%)
  Error (loss) value: 8.439505E-01</code></pre>
			da cui si evince che l&apos;errore calcolato sul dataset di test resta comunque basso ($8.439505 \cdot 10^{-1}$) anche se un po&apos; pi&ugrave; alto del modello originale come ci si aspettava
			in quanto il pruning riduce la qualit&agrave; del modello; i pesi uguali a zero sono 12999 su 26321 (quindi quasi la met&agrave;), il fattore di compressione sul file .h5 &egrave; del $48.02 \%$
			mentre quello sul file .tflite &egrave; del $49.91 \%$.<br />
			L&apos;immagine seguente mostra a colpo d&apos;occhio che la qualit&agrave; del forecast &egrave; relativamente buona.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example2-attempt-pruned.png" alt="Dataset di test e il forecast prodotto con il modello poly decay 40/50"/></div>
			<div class="photocaption">Dataset di test (in verde) e il forecast (in rosso) prodotto con il modello <em>poly decay 40/50</em>.</div>
			<br/>
			Terminati tutti i tentativi, lo script di esempio mostra il recap di tutti i tentativi; il primo elemento del recap &egrave; relativo al modello originale.<br />
			<br/>
			<pre><code class="shell">*** Final recap ***
Attempt name              Size h5 (Comp. %)     Error (loss)
original (unpruned)         99952 ( 18.14%)     1.289916e-01
poly decay 10/50            62316 ( 48.96%)     8.029442e-01
poly decay 20/50            62268 ( 49.00%)     1.122432e-01
poly decay 30/60            53409 ( 56.26%)     2.103334e+00
poly decay 30/70            43866 ( 64.07%)     3.067956e+00
poly decay 40/50            63464 ( 48.02%)     8.439505e-01
poly decay 10/90            22154 ( 81.86%)     4.263138e+00
const sparsity 0.1          96429 ( 21.02%)     2.983370e+00
const sparsity 0.4          72670 ( 40.48%)     3.378339e+00
const sparsity 0.5          63657 ( 47.86%)     3.714817e-01
const sparsity 0.6          54506 ( 55.36%)     4.406884e+00
const sparsity 0.9          22818 ( 81.31%)     4.847150e+00</code></pre>
			da cui si evince che in linea di principio, all&apos;aumentare del fattore di compressione aumenta l&apos;errore calcolato sul dataset di test
			e quindi diminuisce la qualit&agrave; del forecast.<br />
			A titolo di campione l&apos;immagine seguente mostra un modello che ha subito un pesante pruning e conseguentemente la qualit&agrave; dell&apos;inferenza
			&egrave; notevolmente peggiorata rispetto al modello precedentemente mostrato.<br />
			<br />
			<br />
			<div class="betweentextlines"><img src="../../posts/neural-networks/pruning-of-neural-networks-with-tensorflow/nn-prunnnwtf-example2-attempt-weak.png" alt="Dataset di test e forecast prodotto con il modello const spartsity 0.4" /></div>
			<div class="photocaption">Dataset di test (in verde) e forecast (in rosso) prodotto con il modello <em>const spartsity 0.4</em>.</div>
			<br/>
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.<br/>
		</p>

        <h3>Esempio #3: rete neurale convoluzionale</h3>
		<p>
			Il codice di questo esempio &egrave; il file <code><a href="https://github.com/ettoremessina/nn-optimization/blob/main/pruning/demos/tensorflow/example3.py" target="_blank">example3.py</a></code>.<br />
			Il dataset utilizzato da questo esempio &egrave; il dataset <a href="https://www.tensorflow.org/datasets/catalog/tf_flowers" target="_blank">Flowers di TensorFlow</a>;
			qui il codice che effettua il download di tale dataset:
			<pre><code class="python">(train_ds, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

total_number_of_training_images = 8
total_number_of_validation_images = 6
total_number_of_test_images = 7</code></pre>
			Per eseguire questo scripy Python lanciare il seguente comando:<br />
			<br/>
			<pre><code class="shell">$ python example3.py</code></pre>
			Si consiglia di eseguire questo script avendo a disposizione una GPU, anche modesta, in quanto eseguito su CPU pu&ograve; richiedere tanto tempo.<br />
			Nell&apos;output ottenuto, proprio all&apos;inizio, si osserva la struttura del modello, che &egrave; una rete con una serie di layer Conv2D e MaxPooling2D
			seguiti da un layer Dense, poi un Flatten, poi un Dropout (per evitare l&apos;overfitting) e infine un Dense; ha con un totale di 1658565 pesi addestrabili.<br />
			Qui la struttura della rete:<br />
			<br />
			<pre><code class="shell">Model: "cnn_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 180, 180, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 178, 178, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 87, 87, 32)        9248      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 43, 43, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 41, 41, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 20, 20, 32)        0         
_________________________________________________________________
flatten (Flatten)            (None, 12800)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               1638528   
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 645       
=================================================================
Total params: 1,658,565
Trainable params: 1,658,565
Non-trainable params: 0
_________________________________________________________________</code></pre>
			Segue l&apos;addestramento del modello originale che consiste in 15 epoche con batch size di 100 elementi,
			con ottimizzatore <code>Adams</code> e <code>SparseCategoricalCrossentropy(from_logits=True)</code> quale funzione di loss.<br/>
			<b>Nota</b>: Il valore dell&apos; errore mostrato nell&apos;output del programma tuttavia non &egrave; il valore della funzione di loss ottenuta confrontando il dataset di test con la predizione,
			come era per per gli esempi #1 e #2, ma &egrave; il numero delle immagini classificate correttamente diviso il numero totale di immagini del dataset di test.<br/>
			Le propriet&agrave; di questo modello (che chiamiamo modello originale in quanto non sottoposto a pruning)
			sono visibili nell&apos;output del programma:<br />
			<br/>
			<pre><code class="shell">Model: original (unpruned)
  Total number of weights: 1658565
  Total number of non-zero weights: 1658565
  Total number of zero weights: 0
  Unzipped h5 size: 6665816 bytes
  Zipped h5 size: 6151486 bytes (compression factor: 7.72%)
  Unzipped tflite size: 1669728 bytes
  Zipped tflite size: 1358843 bytes (compression factor: 18.62%)
  Error (loss) value: 3.950954E-01</code></pre>
			da cui si evince che l&apos;errore calcolato sulla classificazione sul dataset di test &egrave; $3.950954 \cdot 10^{-1}$, nessun peso uguale a zero come ci si aspettava
			in quanto quello modello non &egrave; stato sottoposto a pruning, il fattore di compressione sul file .h5 &egrave; del $7.72 \%$
			mentre quello sul file .tflite &egrave; del $18.62 \%$.<br />
			<br/>
			Segue quindi l&apos;esecuzione della griglia di ricerca che esegue 11 tentativi di applicazione di pruning, 6 con <code>PolynomialDecay</code> variamente inizializzati
			e 5 con <code>ConstantSparsity</code> variamente inizializzati. A titolo di campione, si mostra il risultato di uno degli 11 tentativi, precisamente <em>poly decay 40/50</em>
			(comunque i risultati di tutti i tentativi sono disponibili sullo standard output):<br />
			<br/>
			<pre><code class="shell">Model: poly decay 40/50
  Total number of weights: 1658565
  Total number of non-zero weights: 829624
  Total number of zero weights: 828941
  Unzipped h5 size: 6665816 bytes
  Zipped h5 size: 3859572 bytes (compression factor: 42.10%)
  Unzipped tflite size: 1669728 bytes
  Zipped tflite size: 978984 bytes (compression factor: 41.37%)
  Error (loss) value: 4.087193E-01</code></pre>
			da cui si evince che l&apos;errore calcolato sul dataset di test resta comunque vicino a quello del modello originale ($4.087193 \cdot 10^{-1}$) anche se un po&apos; pi&ugrave; alto come ci si aspettava
			in quanto il pruning riduce la qualit&agrave; del modello; i pesi uguali a zero sono 828941 su 1658565 (quindi quasi la met&agrave;), il fattore di compressione sul file .h5 &egrave; del $42.10 \%$
			mentre quello sul file .tflite &egrave; del $41.37 \%$.<br />
			<br/>
			Terminati tutti i tentativi, lo script di esempio mostra il recap di tutti i tentativi; il primo elemento del recap &egrave; relativo al modello originale.<br />
			<br/>
			<pre><code class="shell">*** Final recap ***
Attempt name              Size h5 (Comp. %)     Error (loss)
original (unpruned)       6151486 (  7.72%)     3.950954e-01
poly decay 10/50          3781226 ( 43.27%)     4.223433e-01
poly decay 20/50          3786548 ( 43.19%)     4.168937e-01
poly decay 30/60          3238918 ( 51.41%)     4.523161e-01
poly decay 30/70          2602091 ( 60.96%)     4.441417e-01
poly decay 40/50          3859572 ( 42.10%)     4.087193e-01
poly decay 10/90          1286371 ( 80.70%)     4.741144e-01
const sparsity 0.1        5891419 ( 11.62%)     4.604905e-01
const sparsity 0.4        4400747 ( 33.98%)     4.386921e-01
const sparsity 0.5        3830963 ( 42.53%)     4.414169e-01
const sparsity 0.6        3246370 ( 51.30%)     4.441417e-01
const sparsity 0.9        1285383 ( 80.72%)     4.659401e-01</code></pre>
			da cui si evince che in linea di principio, all&apos;aumentare del fattore di compressione aumenta l&apos;errore calcolato sul dataset di test
			e quindi diminuisce la qualit&agrave; della classificazione.<br />
			<br/>
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.<br/>
		</p>

        <h2 id="downloadcode">Download del codice completo</h2>
		<p>
			Il codice completo &egrave; disponibile su <a target="_blank" href="https://github.com/ettoremessina/nn-optimization/tree/main/pruning/demos/tensorflow/">GitHub</a>.
			<br/>
			
			Questo materiale &egrave; distribuito su licenza MIT; sentiti libero di usare, condividere, &quot;forkare&quot; e adattare tale materiale come credi.
			<br/>
			Sentiti anche libero di pubblicare pull-request e bug-report su questo repository di GitHub oppure di contattarmi sui miei canali social disponibili nell&apos;angolo in alto a destra di questa pagina. 
			<br/>

		</p>

	</section>

						</div>
				</div>
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<div class="align-center"><img src="../../images/cm-logo-small.png" alt="Mentalit&agrave;&nbsp;Computazionale"/></div>
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="../../it/">Home</a></li>
										<li>
											<span class="opener">Reti&nbsp;Neurali</span>
											<ul>
												<li><a href="../../it/reti-neurali/">INDICE</a></li>
												<li><a href="../../it/reti-neurali/pruning-di-reti-neurali-con-tensorflow.html">Pruning di reti neurali con TensorFlow</a></li>
												<li><a href="../../it/reti-neurali/equazioni-differenziali-e-reti-neurali.html">Equazioni Differenziali e Reti Neurali</a></li>
												<li><a href="../../it/reti-neurali/forecast-di-una-serie-temporale-univariata-equispaziata-con-tensorflow.html">Forecast di una serie temporale univariata ed equispaziata con TensorFlow</a></li>
												<li><a href="../../it/reti-neurali/approssimazione-con-percettroni-multistrato-altamente-configurabili.html">Approssimazione con percettroni multistrato altamente configurabili</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Computazione&nbsp;Quantistica</span>
											<ul>
												<li><a href="../../it/computazione-quantistica/">INDICE</a></li>
												<li><a href="../../it/computazione-quantistica/operatori-not-cnot.html">Porte quantistiche NOT e C-NOT</a></li>
												<li><a href="../../it/computazione-quantistica/generazione-numero-casuale.html">Generazione di un numero casuale</a></li>
												<li><a href="../../it/computazione-quantistica/porte-hadamard-in-cascata.html">Porte Hadamard in cascata</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Machine&nbsp;Learning</span>
											<ul>
												<li><a href="../../it/machine-learning/">INDICE</a></li>
												<li><a href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html">Strumenti generali per l&apos;approssimazione di funzioni</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-pycaret.html">Approssimazione di funzioni tramite PyCaret</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html">Approssimazione di funzioni tramite un regressore XGBoost configurabile</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-svr-configurabile.html">Approssimazione di funzioni tramite un Support Vector Regressor configurabile</a></li>
												<li><a href="../../it/machine-learning/regressione-polinomiale-con-accord-net.html">Regressione polinomiale con Accord.NET</a></li>
												<li><a href="../../it/machine-learning/regressione-smo-con-kernel-puk-in-weka.html">Regressione con SMO per SVM con kernel PUK in Weka</a></li>
												<li><a href="../../it/machine-learning/forecast-smo-con-kernel-polinomiale-in-weka.html">Forecast con SMO per SVM con kernel polinomiale in Weka</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Matematica</span>
											<ul>
												<li><a href="../../it/matematica/">INDICE</a></li>
												<li><a href="../../it/matematica/serie-di-fourier-in-python.html">Serie di Fourier in Python</a></li>
												<li><a href="../../it/matematica/risoluzione-equazioni-differenziali-con-ritardo-in-python-con-metodi-numerici.html">Risoluzione di equazioni differenziali con ritardo con metodi numerici in Python</a></li>
												<li><a href="../../it/matematica/calcolo-integrale-in-python.html">Calcolo Integrale in Python</a></li>
												<li><a href="../../it/matematica/analizzatore-di-un-sistema-dinamico-sul-piano-lineare-omogeneo-coefficienti-costanti.html">Analizzatore di un sistema dinamico, lineare e omogeneo sul piano a coefficienti costanti</a></li>
												<li><a href="../../it/matematica/analizzatore-di-un-sistema-dinamico-non-lineare-autonomo-sul-piano-tramite-teorema-hartman-grobman.html">Analizzatore di un sistema dinamico non lineare e autonomo sul piano tramite il teorema di Hartman-Grobman</a></li>
												<li><a href="../../it/matematica/esperimenti-con-sympy-per-risolvere-odes-ordine-1.html">Esperimenti con SymPy per risolvere equazioni differenziali ordinarie del 1&deg; ordine</a></li>
												<li><a href="../../it/matematica/metodo-soluzione-dde-primo-ordine-usando-funzione-w-lambert.html">Un metodo di soluzione di una equazione differenziale con ritardo del primo ordine utilizzando la funzione W di Lambert</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">High Performance</span>
											<ul>
												<li><a href="../../it/high-performance/">INDICE</a></li>
												<li><a href="https://ettoremessina.tech/2024/10/03/high-performance-computing-of-discrete-two-variable-partial-derivatives-with-cuda/" target="_blank">High-Performance Computing of Discrete Single Variable Derivative with CUDA<br />
(post disponibile solo in lingua inglese)</a></li>
												<li><a href="https://ettoremessina.tech/2024/10/03/high-performance-computing-of-discrete-two-variable-partial-derivatives-with-cuda/" target="_blank">High-Performance Computing of Discrete Single Variable Derivative with CUDA<br />
(post disponibile solo in lingua inglese)</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Dataset</span>
											<ul>
												<li><a href="../../it/datasets/">INDICE</a></li>
												<li><a href="../../it/datasets/functions-dataset.html">Collezione di dataset &apos;Functions&apos;</a></li>
												<li><a href="../../it/datasets/time-series-dataset.html">Collezione di dataset &apos;Time&nbsp;Series&apos;</a></li>
												<li><a href="../../it/datasets/synthetic-words-dataset.html">Dataset &apos;Synthetic Words&apos;</a></li>
											</ul>
										</li>
										<li><a href="../../it/info.html">Info</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
                                    <div class="side-topics">
                                        <header class="align-center">
                                            <h2><a href="../../it/reti-neurali/">Reti&nbsp;Neurali</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/reti-neurali/" class="image"><span class="icon solid fa-sitemap"/></a>
                                        </article>
                                        <header class="align-center">
                                            <h2><a href="../../it/computazione-quantistica/">Computazione&nbsp;Quantistica</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/computazione-quantistica/" class="image"><span class="icon solid fa-atom"/></a>
                                        </article>
                                    </div>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">
										Grafica basata sul template &apos;Editorial&apos; (con personalizzazione) scaricato da <a href="https://html5up.net" target="_blank">HTML5 UP</a>.
										<br/>
										Clicka sui link per vedere i file <a href="../../html5up-license/LICENSE.txt" target="_blank">LICENSE.txt</a> e <a href="../../html5up-license/README.txt" target="_blank">README.txt</a> del template &apos;Editorial&apos; di HTML5 UP.
										<br>
										<br>
										&copy; <a href="../../it/info.html">Ettore Messina</a>. 
									</p>
								</footer>
						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

			<style>
				a.cc-link
				{
			    	border-bottom: none;
				}
				a.cc-link:hover
				{
					color: white !important;
				}
			</style>
	</body>
</html>


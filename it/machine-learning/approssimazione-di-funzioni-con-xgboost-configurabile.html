
<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="it">
	<head>
		<meta http-equiv="content-language" content="it">
		<meta name="author" content="Ettore Messina">

		<style>
			#cookiescript_checkbox_input {
				-moz-appearance: checkbox;
				-webkit-appearance: checkbox;
				-ms-appearance: checkbox;
				appearance: checkbox;
				opacity: 1.0;
			}
			#cookiescript_checkbox_text {
				color: white;
			}
			#cookiescript_description a:hover {
				color: yellow !important;
			}
		</style>
		<script type="text/javascript" charset="UTF-8" src="https://cookie-script.com/s/19e1626ea9f21a6fcc285b559b5957e6.js"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance" src="https://www.googletagmanager.com/gtag/js?id=UA-149444322-1"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance">
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'UA-149444322-1', { 'anonymize_ip': true });
		</script>


		<title>Approssimazione di funzioni tramite un regressore XGBoost configurabile</title>
		<meta name="description" content="Approssimazione di funzioni tramite un regressore XGBoost configurabile" >
		<meta name="keywords" content="regressione, xgboost, approssimazione, funzione, dataset, machine learning" >
		<link rel="canonical" href="https://computationalmindset.com/it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html" />
		<link rel="alternate" hreflang="en" href="https://computationalmindset.com/en/machine-learning/fitting-with-configurable-xgboost.html" />
		<link rel="alternate" hreflang="it" href="https://computationalmindset.com/it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html" />
		
    <!-- SCHEMA.ORG JSON-LD WEBSITE -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "name": "Computational Mindset",
        "url": "https://computationalmindset.com/",
        "sameAs": ["https://www.facebook.com/ComputationalMindset/", "https://www.facebook.com/MentalitaComputazionale/", "https://github.com/ettoremessina/"],
        "author":
        {
          "@type": "Person",
          "name": "Ettore Messina",
          "image": "https://computationalmindset.com/images/ettore-messina.jpg",
          "gender": "Male",          
          "sameAs": ["https://www.facebook.com/ettore.messina.73/", "https://www.instagram.com/etmessina/", "https://twitter.com/ettoremessina/", "https://github.com/ettoremessina/", "https://medium.com/@ettoremessina/", "https://www.linkedin.com/in/ettoremessina/"]
        }
    }
    </script>

		
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement":
        [
		
          {
            "@type": "ListItem",
            "position": 1,
            "item":
            {
                "@id": "https://computationalmindset.com/it/",
                "name": "Mentalit&agrave; Computazionale"
            }
          },
		
          {
            "@type": "ListItem",
            "position": 2,
            "item":
            {
              "@id": "https://computationalmindset.com/it/machine-learning/",
              "name": "Machine Learning"
            }
          },

          {
            "@type": "ListItem",
            "position": 3,
            "item":
            {
              "@id": "https://computationalmindset.com/it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html",
              "name": "Approssimazione di funzioni tramite un regressore XGBoost configurabile"
            }
		  }
		
        ]
    }
    </script>
		

		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/railscasts.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
		
		<script>hljs.initHighlightingOnLoad();</script>
		<style>
			pre > code 
			{
				font-size: 1.2em;
			}
		</style>

		
		<script type="text/javascript" src="https://latex.codecogs.com/latexit.js"></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>



		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
		<link rel="icon" href="../../favicon.ico" type="image/x-icon" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../it/info.html" class="logo"><strong>Mentalit&agrave;&nbsp;Computazionale</strong> di&nbsp;Ettore&nbsp;Messina</a>
									<div style="text-align:right">
										<a class="logo" href="../../en/">en</a>
										&nbsp;&nbsp;&nbsp;
										<a class="logo" href="../../it/">it</a>
									</div>
									<ul class="icons">
										<li><a href="https://github.com/ettoremessina/" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.facebook.com/MentalitaComputazionale/" class="icon brands fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.youtube.com/channel/UCKrOtSEJjs5msOhPIdYEeWA/" class="icon brands fa-youtube" target="_blank"><span class="label">YouTube</span></a></li>
										<li><a href="https://www.linkedin.com/in/ettoremessina/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://medium.com/@ettoremessina/" class="icon brands fa-medium-m" target="_blank"><span class="label">Medium</span></a></li>
										<li><a href="https://linktr.ee/ComputationalMindset/" class="fas fa-link" style="color: grey;" target="_blank"><span class="label"></span></a></li>
									</ul>
								</header>

<!-- Content -->
	<section>
		<header class="main">
			<h1>Approssimazione di funzioni tramite un regressore XGBoost configurabile</h1>
		</header>

		<p>
			Questo post tratta dell&apos;approssimazione di funzioni matematiche, sia scalari che vettoriali, sia a una che pi&ugrave; variabili reali tramite un regressore <em>XGBoost</em>
			senza scrivere codice ma agendo solamente sulla linea di comando di script Python che implementano le funzionalit&agrave; di:
			
			<ul>
				<li>
					<em>Configurazione del regressore e addestramento</em>
				</li>
				<li>
					<em>Predizione e calcolo dell&apos;errore</em>
				</li>
			</ul>

			Il codice descritto da questo post richiede la versione 3 di Python e utilizza la libreria <em>XGBoost</em>; richiede inoltre le librerie SciKit-Learn, NumPy, Pandas, MatPlotLib e JobLib.
			Per ottenere il codice si veda il paragrafo <a href="#downloadcode">Download del codice completo</a> in fondo a questo post.<br/>
			<br/>
			Per quanto riguarda la generazione dei dataset sintetici di training e di test si far&agrave; uso dei seguenti strumenti (presenti nel repository):
			<ul>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fx_gen"><code>fx_gen.py</code></a> per le funzioni generatrici scalari reali di una variabile reale $f(x) \colon [a,b] \to {\rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fxy_gen"><code>fxy_gen.py</code></a> per le funzioni generatrici scalari reali di due variabili reali $f(x,y) \colon [a,b] \times [c,d] \to {\rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#pmc2t_gen"><code>pcm2t_gen.py</code></a> per le curve parametriche sul piano, quindi funzioni vettoriali reali $f(t) \colon [a,b] \to {\rm I\!R \times \rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#pmc3t_gen"><code>pmc3t_gen.py</code></a> per le curve parametriche nello spazio, quindi funzioni vettoriali reali $f(t) \colon [a,b] \to {\rm I\!R \times \rm I\!R \times \rm I\!R}$
				</li>
			</ul>
			Per la visualizzazione dei risultati, e precisamente per la comparazione del dataset di test con la predizione, si far&agrave; uso dei seguenti strumenti (sempre presenti nel repository):
			<ul>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fx_scatter"><code>fx_scatter.py</code></a> per le funzioni scalari reali di una variabile reale
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fxy_scatter"><code>fxy_scatter.py</code></a> per le funzioni scalari reali di due variabili reali
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#pmc2t_scatter"><code>pmc2t_scatter.py</code></a> per le curve parametriche sul piano
				</li>
				<li>
					<a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#pmc3t_scatter"><code>pmc3t_scatter.py</code></a> per le curve parametriche nello spazio
				</li>
			</ul>
		</p>

		<h2>Configurazione del regressore e addestramento</h2>
		<p>
			In questo capitolo sono presentati i programmi
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py"><code>fit_func_miso.py</code></a> e
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_mimo.py"><code>fit_func_mimo.py</code></a>
			che sono tecnicamente wrapper della classe <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRFRegressor"><code>XGBRFRegressor</code></a>
			della libreria <em>XGBoost</em> e il cui fine &egrave; di consentire l&apos;utilizzazione del regressore sottostante per approssimare funzioni
			senza dover scrivere codice ma agendo solo sulla linea di comando.<br />
			Infatti tramite l&apos;argomento <code>--xgbparams</code> l&apos;utente passa una serie di iper parametri per regolare il comportamento dell&apos;algoritmo del regressore XGBoost sottostante
			e altri per configurare l&apos;apprendimento dello stesso.
			Oltre ai parametri del regressore sottostante i due programmi supportano dei propri argomenti per permettere all&apos;utente di passare
			il dataset di training ed eventualmente quello di validazione, su quale file salvare il modello addestrato, le metriche da calcolare durante il training,
			vincoli per la regolarizzazione (ad esempio l&apos;<em>early stop</em>) e parametrri per la diagnostica.<br />
			<br />
			Il programma <code>fit_func_miso.py</code>, cos&igrave; come il regressore XGBoost sottostante, &egrave; di tipo <em>M.I.S.O.</em>, cio&egrave; <em>Multiple Input Single Output</em>:
			&egrave; progettato per approssimare una funzione scalre della forma $f \colon \rm I\!R^n \to \rm I\!R$  dove il numero delle variabili indipendenti &egrave; arbitrariamente grande
			mentre la variabile dipendete in uscita &egrave; solamente una.<br />
			Il formato dei dataset in ingresso &egrave; in formato csv (con header), con n + 1 colonne, di cui le prime n contengono i valori delle n variabili indipendenti e
			l&apos;ultima colonna, la n+1, contenente i valori della variabile dipendente.
			<br />
			Il programma <code>fit_func_mimo.py</code>, utilizzando nell&apos;implementazione la classe <a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor"><code>sklearn.multioutput.MultiOutputRegressor</code></a>
			&egrave; di tipo <em>M.I.M.O.</em>, cio&egrave; <em>Multiple Input Multiple Output</em>:
			&egrave; progettato per approssimare una funzione vettoriale della forma $f \colon \rm I\!R^n \to \rm I\!R^m$ dove il numero delle variabili indipendenti &egrave; arbitrariamente grande
			mentre la variabile dipendete in uscita &egrave; solamente una.<br />
			Il formato dei dataset in ingresso &egrave; in formato csv (con header), con $n + m$ colonne, di cui le prime $n$ colonne contengono i valori delle $n$ variabili indipendenti e
			le ultime $m$ contenenti i valori della variabili dipendenti.
		</p>

		<h3 id="fit_func_miso">Usage del programma fit_func_miso.py</h3>
		<p>
			Per ottenere l&apos;usage del programma &egrave; sufficiente eseguire il seguente comando:
			<pre><code class="shell">$ python fit_func_miso.py --help</code></pre>
			e l&apos;output ottenuto &egrave;:
			<br/>
			<br/>
			<pre><code class="shell">usage: fit_func_miso.py [-h] [--version] --trainds TRAIN_DATASET_FILENAME
                        --modelout MODEL_FILE [--valds VAL_DATASET_FILENAME]
                        [--metrics VAL_METRICS [VAL_METRICS ...]]
                        [--dumpout DUMPOUT_PATH]
                        [--earlystop EARLY_STOPPING_ROUNDS]
                        [--xgbparams XGB_PARAMS]

fit_func_miso.py fits a multiple-input single-output scalar function dataset
using a configurable XGBoost

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --trainds TRAIN_DATASET_FILENAME
                        Train dataset file (csv format)
  --modelout MODEL_FILE
                        Output model file
  --valds VAL_DATASET_FILENAME
                        Validation dataset file (csv format)
  --metrics VAL_METRICS [VAL_METRICS ...]
                        List of built-in evaluation metrics to apply to
                        validation dataset
  --dumpout DUMPOUT_PATH
                        Dump directory (directory to store metric values)
  --earlystop EARLY_STOPPING_ROUNDS
                        Number of round for early stopping
  --xgbparams XGB_PARAMS
                        Parameters of XGBoost constructor</code></pre>
			Dove:
			<ul>
				<li>
					<b>-h, --help</b>: mostra l&apos;usage del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: mostra la versione del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--trainds</b>: percorso (relativo o assoluto) del file csv a due colonne (con header) che contiene il dataset da utilizzare per il training;
					questo file pu&ograve; essere generato in modo sintetico ad esempio tramite il programma <a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fx_gen"><code>fx_gen.py</code></a>
					oppure essere un dataset ottenuto realmente misurando un fenomeno scalare e reale che dipende da una sola variabile reale.<br />
					<br />
				</li>
				<li>
					<b>--modelout</b>: percorso (relativo o assoluto) al file dove salvare il modello addestrato nel formato joblib (.jl).<br />
					<br />
				</li>
				<li>
					<b>--valds</b>: percorso (relativo o assoluto) del file csv a due colonne (con header) che contiene il dataset da utilizzare per il validation.<br />
					<br />
				</li>
				<li>
					<b>--metrics</b>: lista di metriche da calcolare sul dataset di training e, se presente, anche su quello di validation;
					la lista delle metriche supportate &egrave; definita in <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/parameter.html">XGBoost Parameters</a>
					alla voce <em>eval_metric</em>.<br />
					<br />
				</li>
				<li>			
					<b>--dumpout</b>: percorso (relativo o assoluto) della directory dove salvare i valori delle metriche allo scorrere delle epoche;
					il programma <a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#dumps_scatter"><code>dumps_scatter.py</code></a> utilizzer&agrave; il contenuto di questa directory per visualizzare i grafici delle metriche.<br />
					<br />
				</li>
				<li>
					<b>--earlystop</b>: quante iterazioni possono essere eseguite prima che l&apos;algoritmo inizi a entrare nella fase di overfitting.<br />
					<br />
				</li>
				<li>
					<b>--xgbparams</b>: lista di parametri da passare all&apos;algoritmo di regressione di XGBoost; si veda la documentazione di <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"><code>XGBRegressor</code></a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h3 id="fit_func_mimo">Usage del programma fit_func_mimo.py</h3>
		<p>
			Per ottenere l&apos;usage del programma &egrave; sufficiente eseguire il seguente comando:
			<pre><code class="shell">$ python fit_func_mimo.py --help</code></pre>
			e l&apos;output ottenuto &egrave;:
			<br/>
			<br/>
			<pre><code class="shell">usage: fit_func_mimo.py [-h] [--version] --trainds TRAIN_DATASET_FILENAME
                        --outputdim NUM_OF_DEPENDENT_COLUMNS --modelout
                        MODEL_FILE [--dumpout DUMPOUT_PATH]
                        [--xgbparams XGB_PARAMS]

fit_func_mimo.py fits a multiple-input single-output (scalar) function dataset
using a configurable XGBoost

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --trainds TRAIN_DATASET_FILENAME
                        Train dataset file (csv format)
  --outputdim NUM_OF_DEPENDENT_COLUMNS
                        Output dimension (alias the number of dependent
                        columns, that must be last columns)
  --modelout MODEL_FILE
                        Output model file
  --dumpout DUMPOUT_PATH
                        Dump directory (directory to store metric values)
  --xgbparams XGB_PARAMS
                        Parameters of XGBoost constructor</code></pre>
			Dove:
			<ul>
				<li>
					<b>-h, --help</b>: mostra l&apos;usage del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: mostra la versione del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--trainds</b>: percorso (relativo o assoluto) del file csv a due colonne (con header) che contiene il dataset da utilizzare per il training;
					questo file pu&ograve; essere generato in modo sintetico ad esempio tramite il programma <a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#fx_gen"><code>fx_gen.py</code></a>
					oppure essere un dataset ottenuto realmente misurando un fenomeno scalare e reale che dipende da una sola variabile reale.<br />
					<br />
				</li>
				<li>
					<b>--outputdim</b>:
						il numero $n$ delle variabili indipendenti che devono corrispondere alle prime $n$ colonne del dataset csv;
						il resto delle colonne a destra sono considerante di conseguenza la $m$ variabili dipendenti.<br />
					<br />
				</li>
				<li>
					<b>--modelout</b>: percorso (relativo o assoluto) al file dove salvare il modello addestrato nel formato joblib (.jl).<br />
					<br />
				</li>
				<li>			
					<b>--dumpout</b>: percorso (relativo o assoluto) della directory dove salvare i valori delle metriche allo scorrere delle epoche;
					il programma <a target="_blank" href="../../it/machine-learning/strumenti-generali-per-approssimazione-di-funzioni.html#dumps_scatter"><code>dumps_scatter.py</code></a> utilizzer&agrave; il contenuto di questa directory per visualizzare i grafici delle metriche.<br />
					<br />
				</li>
				<li>
					<b>--xgbparams</b>: lista di parametri da passare all&apos;algoritmo di regressione di XGBoost; si veda la documentazione di <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"><code>XGBRegressor</code></a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h2>Predizione e calcolo dell&apos;errore</h2>
		<p>
			In questo capitolo sono invece presentati i programmi
			<a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/predict_func_miso.py" target="_blank"><code>predict_func_miso.py</code></a> e
			<a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/predict_func_mimo.py" target="_blank"><code>predict_func_mimo.py</code></a>
			il cui scopo &egrave; quello di effettuare le predizioni su un dataset di test applicandolo a un modello di regressore XGBoost addestrato precedentemente
			rispettivamente tramite il programma
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py"><code>fit_func_miso.py</code></a> e
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_mimo.py"><code>fit_func_mimo.py</code></a>,
			sempre senza dover scrivere codice ma tramite la sola linea di comando.<br />
			Infatti questi due programmi supportano argomenti tramite i quali l&apos;utente passa il modello addestrato precentemente, il dataset di test
			e le misure di errore da calcolare tra la predizione e il valore vero.<br />
			Il formato dei dataset di test in ingresso &egrave; rispettivamente identico a quello dei programmi <code>fit_func_miso.py</code> e <code>fit_func_mimo.py</code>;
			ovviamente le ultime colonne (relative alle variabili dipendenti) sono adoperate solo per confrontare i valori predetti con i valori veri con i valori veri calcolando le misure di errore passate.<br />
		</p>

		<h3 id="predict_func_miso">Usage del programma predict_func_miso.py</h3>
		<p>
			Per ottenere l&apos;usage del programma &egrave; sufficiente eseguire il seguente comando:
			<pre><code class="shell">$ python predict_func_miso.py --help</code></pre>
			e l&apos;output ottenuto &egrave;:
			<br/>
			<br/>
			<pre><code class="shell">usage: predict_func_miso.py [-h] [--version] --model MODEL_FILE --ds
                            DF_PREDICTION --predictionout PREDICTION_DATA_FILE
                            [--measures MEASURES [MEASURES ...]]

predict_func_miso.py makes prediction of the values of a multiple-input
single-output (scalar) function with a pretrained XGBoost model

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --model MODEL_FILE    model file
  --ds DF_PREDICTION    dataset file (csv format)
  --predictionout PREDICTION_DATA_FILE
                        prediction data file (csv format)
  --measures MEASURES [MEASURES ...]
                        List of built-in sklearn regression metrics to compare
                        prediction with input dataset</code></pre>
			Dove:
			<ul>
				<li>
					<b>-h, --help</b>: mostra l&apos;usage del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: mostra la versione del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--model</b>: percorso (relativo o assoluto) al file nel formato joblib (.jl) del modello generato da <code>fit_func_miso.py</code>.<br />
					<br />
				</li>
				<li>
					<b>--ds</b>: percorso (relativo o assoluto) del file csv (con header) che contiene il dataset di input su cui calcolare la predizione.<br />
					<br />
				</li>
				<li>
					<b>--predictionout</b>: percorso (relativo o assoluto) del file csv da generare che conterr&agrave; la predizione, ovverosia l&apos;approssimazione della funzione applicata al dataset di input.<br />
					<br />
				</li>
				<li>
					<b>--measures</b>: lista di misure da calcolare confrontando i valori veri del dataset di input e i valori predetti in uscita;
					la lista delle metriche supportate &egrave; definita in <a target="_blank" href="https://scikit-learn.org/stable/modules/classes.html#regression-metrics">SciKit Learn Regression Metrics</a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h3 id="predict_func_mimo">Usage del programma predict_func_mimo.py</h3>
		<p>
			Per ottenere l&apos;usage del programma &egrave; sufficiente eseguire il seguente comando:
			<pre><code class="shell">$ python predict_func_mimo.py --help</code></pre>
			e l&apos;output ottenuto &egrave;:
			<br/>
			<br/>
			<pre><code class="shell">usage: predict_func_mimo.py [-h] [--version] --model MODEL_FILE --ds
                            DF_PREDICTION --outputdim NUM_OF_DEPENDENT_COLUMNS
                            --predictionout PREDICTION_DATA_FILE
                            [--measures MEASURES [MEASURES ...]]

predict_func_mimo.py makes prediction of the values of a multiple-input
single-output (scalar) function with a pretrained XGBoost model

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --model MODEL_FILE    model file
  --ds DF_PREDICTION    dataset file (csv format)
  --outputdim NUM_OF_DEPENDENT_COLUMNS
                        Output dimension (alias the number of dependent
                        columns, that must be last columns)
  --predictionout PREDICTION_DATA_FILE
                        prediction data file (csv format)
  --measures MEASURES [MEASURES ...]
                        List of built-in sklearn regression metrics to compare
                        prediction with input dataset</code></pre>
			Dove:
			<ul>
				<li>
					<b>-h, --help</b>: mostra l&apos;usage del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: mostra la versione del programma e termina l&apos;esecuzione.<br />
					<br />
				</li>
				<li>
					<b>--model</b>: percorso (relativo o assoluto) al file nel formato joblib (.jl) del modello generato da <code>fit_func_miso.py</code>.<br />
					<br />
				</li>
				<li>
					<b>--ds</b>: percorso (relativo o assoluto) del file csv (con header) che contiene il dataset di input su cui calcolare la predizione.<br />
					<br />
				</li>
				<li>
					<b>--outputdim</b>:
						il numero $n$ delle variabili indipendenti che devono corrispondere alle prime $n$ colonne del dataset csv;
						il resto delle colonne a destra sono considerante di conseguenza la $m$ variabili dipendenti.<br />
					<br />
				</li>
				<li>
					<b>--predictionout</b>: percorso (relativo o assoluto) del file csv da generare che conterr&agrave; la predizione, ovverosia l&apos;approssimazione della funzione applicata al dataset di input.<br />
					<br />
				</li>
				<li>
					<b>--measures</b>: lista di misure da calcolare confrontando i valori veri del dataset di input e i valori predetti in uscita;
					la lista delle metriche supportate &egrave; definita in <a target="_blank" href="https://scikit-learn.org/stable/modules/classes.html#regression-metrics">SciKit Learn Regression Metrics</a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h2>Un esempio di uso di tutti i programmi</h3>
		<p>
			Si supponga di voler approssimare nell&apos;intervallo $[-0.4,0.4]$ la seguente funzione $$f(x)=x sin \frac{1}{x}$$
			Tenendo presente che <em>np</em> &egrave; l&apos;alias della libreria NumPy, questa si traduce in sintassi lambda body Python cos&igrave;:
			<pre><code class="python">x * np.sin(1 / x)</code></pre>
			Per generare il dataset di training, si esegua quindi il seguente comando:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_gen.py \
  --dsout mytrain.csv \
  --funcx "x * np.sin(1 / x)" \
  --xbegin -0.4 \
  --xend 0.4 \
  --xstep 0.00031</code></pre>
			mentre per generare il dataset di test, si esegua il seguente comando:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_gen.py \
  --dsout mytest.csv \
  --funcx "x * np.sin(1 / x)" \
  --xbegin -0.4 \
  --xend 0.4 \
  --xstep 0.00073</code></pre>
			Si osservi che il passo di discretizzazione del dataset di test &egrave; pi&ugrave; grande di quello di training e questo &egrave; normale 
			in quanto l&apos;addestramento, per essere accurato, deve essere eseguito su una maggiore quantit&agrave; di dati.
			Inoltre si osservi che &egrave; opportuno che il passo di discretizzazione del dataset di test <u>non</u> sia un multiplo di quello di training
			per garantire che il dataset di test contenga la maggior parte dei dati non presenti nel dataset di training, e questo rende pi&ugrave; interessante la predizione.<br />
			<br />
			A questo si intende effettuare una regressione tramite <a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py" target="_blank"><code>fit_func_miso.py</code></a>
			passando al regressore sottostante i seguenti argomenti: n_estimators: 100, max_depth: 7;
			quindi si esegua quindi il seguente comando:
			<br/>
			<br/>
			<pre><code class="shell">$ python fit_func_miso.py \
  --trainds mytrain.csv \
  --modelout mymodel.jl \
  --xgbparams "'n_estimators': 100, 'max_depth': 7"</code></pre>
			e alla fine dell&apos;esecuzione il file mymodel.jl salvato contiene il modello del regressore XGBoost configurato e addestrato.<br />
			<br />
			Adesso si intende effettuare la predizione e il calcolo dell&apos;errore usando le misure <em>mean_absolute_error</em> e <em>mean_squared_error</em>;
			quindi si esegua quindi il seguente comando:
			<br/>
			<br/>
			<pre><code class="shell">$ python predict_func_miso.py \
  --model mymodel.jl \
  --ds mytest.csv \
  --predictionout mypred.csv \
  --measures mean_absolute_error mean_squared_error</code></pre>
  			e alla fine dell&apos;esecuzione il file mypred.csv salvato contiene la predizione eseguita applicando il modello sui dati di test;
			l&apos;output del programma visualizza le misure di errore passate tramite l&apos;argomento <code>--measures</code>
			e sono molto piccole: la prima intorno a $1.5 \cdot 10^{-3}$ e la seconda intorno a $5.5 \cdot 10^{-6}$<br />
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.<br/>
			<br />
			Infine si desidera intende effettuare la visualizzazione comparata del dataset di test con la predizione;
			si esegua perci&ograve; il seguente comando:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_scatter.py \
  --ds mytest.csv \
  --prediction mypred.csv \
  --title "XGBoost (estimators: 100, max depth: 7)" \
  --xlabel "x" \
  --ylabel "y=x sin(1/x)"</code></pre>
			che mostra i grafici a dispersione del dataset di test e la predizione sovrapposti: in blu quello del dataset di test, in rosso la predizione.
			La comparazione dei due grafici evidenzia chiaramente che l&apos;approssimazione ha raggiunto livelli molto elevati, come del resto le basse misure dell&apos;errore gi&agrave; indicavano;
			solamente intorno allo zero si osserva qualche imprecisione ed &egrave; dovuta all&apos;oscillazione della funzione che &egrave; l&igrave; molto forte.<br />
			<br />
			<b>Nota</b>: Data la natura stocastica della fase di addestramento, i singoli specifici risultati possono variare. Si consideri di eseguire la fase di addestramento pi&ugrave; volte.
		</p>
		<div class="betweentextlines"><img src="../../posts/machine-learning/fitting-with-configurable-xgboost/ml-fitfnwcxgb-example10.png" /></div>
		<div class="photocaption">Figura con grafici a dispersione generata dal programma <code>fx_scatter.py</code> che mostra l&apos;approssimazione in sovraimpressione in rosso della funzione $f(x)=x sin \frac{1}{x}$
		e la funzione originale sottostante in blu.</div>
		<br />
		<p>
			Il repository contiene degli esempi in script shell che mostrano l&apos;uso di questi programmi in cascata:
			<ul>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/one-variable-function/xgboost/examples"><code>one-variable-function/xgboost/examples</code></a>
					per esempi sull&apos;approssimazione di funzioni scalari reali di una variabile reale tramite <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/two-variables-function/xgboost/examples"><code>two-variables-function/xgboost/examples</code></a> p
					per esempi sull&apos;approssimazione di funzioni scalari reali di due variabili reali tramite <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/parametric-curve-on-plane/xgboost/examples"><code>parametric-curve-on-plane/xgboost/examples</code></a>
					per esempi sull&apos;approssimazione di curve parametriche sul piano tramite <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/parametric-curve-in-space/xgboost/examples"><code>parametric-curve-in-space/xgboost/examples</code></a>
					per esempi sull&apos;approssimazione di curve parametriche nello spazio tramite <em>XGBoost</em>.
				</li>
			</ul>		
		</p>
		<br/>

		<h2 id="downloadcode">Download del codice completo</h2>
		<p>
			Il codice completo &egrave; disponibile su <a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/xgboost/">GitHub</a>.
			<br/>
			
			Questo materiale &egrave; distribuito su licenza MIT; sentiti libero di usare, condividere, &quot;forkare&quot; e adattare tale materiale come credi.
			<br/>
			Sentiti anche libero di pubblicare pull-request e bug-report su questo repository di GitHub oppure di contattarmi sui miei canali social disponibili nell&apos;angolo in alto a destra di questa pagina. 
			<br/>

		</p>
	</section>

						</div>
				</div>
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<div class="align-center"><img src="../../images/cm-logo-small.png" alt="Mentalit&agrave;&nbsp;Computazionale"/></div>
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="../../it/">Home</a></li>
										<li>
											<span class="opener">Reti&nbsp;Neurali</span>
											<ul>
												<li><a href="../../it/reti-neurali/">INDICE</a></li>
												<li><a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-python-con-tensorflowdiffeq.html">Esperimenti con Neural ODEs in Python con TensorFlowDiffEq</a></li>
												<li><a href="../../it/reti-neurali/esperimenti-con-neural-odes-in-julia.html">Esperimenti con Neural ODEs in Julia</a></li>
												<li><a href="../../it/reti-neurali/risolutori-di-equazioni-differenziali-ordinarie.html">Risolutori di equazioni differenziali ordinarie in Python</a></li>
												<li><a href="../../it/reti-neurali/risolutori-di-equazioni-differenziali-ordinarie-in-julia.html">Risolutori di equazioni differenziali ordinarie in Julia</a></li>
												<li><a href="../../it/reti-neurali/forecast-di-una-serie-temporale-univariata-equispaziata-con-tensorflow.html">Forecast di una serie temporale univariata ed equispaziata con TensorFlow</a></li>
												<li><a href="../../it/reti-neurali/approssimazione-con-percettroni-multistrato-altamente-configurabili.html">Approssimazione con percettroni multistrato altamente configurabili</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Computazione&nbsp;Quantistica</span>
											<ul>
												<li><a href="../../it/computazione-quantistica/">INDICE</a></li>
												<li><a href="../../it/computazione-quantistica/operatori-not-cnot.html">Porte quantistiche NOT e C-NOT</a></li>
												<li><a href="../../it/computazione-quantistica/generazione-numero-casuale.html">Generazione di un numero casuale</a></li>
												<li><a href="../../it/computazione-quantistica/porte-hadamard-in-cascata.html">Porte Hadamard in cascata</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Machine&nbsp;Learning</span>
											<ul>
												<li><a href="../../it/machine-learning/">INDICE</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html">Approssimazione di funzioni tramite un regressore XGBoost configurabile</a></li>
												<li><a href="../../it/machine-learning/approssimazione-di-funzioni-con-svr-configurabile.html">Approssimazione di funzioni tramite un Support Vector Regressor configurabile</a></li>
												<li><a href="../../it/machine-learning/regressione-polinomiale-con-accord-net.html">Regressione polinomiale con Accord.NET</a></li>
												<li><a href="../../it/machine-learning/regressione-smo-con-kernel-puk-in-weka.html">Regressione con SMO per SVM con kernel PUK in Weka</a></li>
												<li><a href="../../it/machine-learning/forecast-smo-con-kernel-polinomiale-in-weka.html">Forecast con SMO per SVM con kernel polinomiale in Weka</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Dataset</span>
											<ul>
												<li><a href="../../it/datasets/">INDICE</a></li>
												<li><a href="../../it/datasets/functions-dataset.html">Collezione di dataset &apos;Functions&apos;</a></li>
												<li><a href="../../it/datasets/time-series-dataset.html">Collezione di dataset &apos;Time&nbsp;Series&apos;</a></li>
												<li><a href="../../it/datasets/synthetic-words-dataset.html">Dataset &apos;Synthetic Words&apos;</a></li>
											</ul>
										</li>
										<li><a href="../../it/info.html">Info</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
                                    <div class="side-topics">
                                        <header class="align-center">
                                            <h2><a href="../../it/reti-neurali/">Reti&nbsp;Neurali</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/reti-neurali/" class="image"><span class="icon solid fa-sitemap"/></a>
                                        </article>
                                        <header class="align-center">
                                            <h2><a href="../../it/computazione-quantistica/">Computazione&nbsp;Quantistica</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../it/computazione-quantistica/" class="image"><span class="icon solid fa-atom"/></a>
                                        </article>
                                    </div>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">
										Grafica basata sul template &apos;Editorial&apos; (con personalizzazione) scaricato da <a href="https://html5up.net" target="_blank">HTML5 UP</a>.
										<br/>
										Clicka sui link per vedere i file <a href="../../html5up-license/LICENSE.txt" target="_blank">LICENSE.txt</a> e <a href="../../html5up-license/README.txt" target="_blank">README.txt</a> del template &apos;Editorial&apos; di HTML5 UP.
										<br>
										<br>
										&copy; <a href="../../it/info.html">Ettore Messina</a>. 
									</p>
								</footer>
						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

			<style>
				a.cc-link
				{
			    	border-bottom: none;
				}
				a.cc-link:hover
				{
					color: white !important;
				}
			</style>
	</body>
</html>


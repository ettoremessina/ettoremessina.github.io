

<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head lang="en">
		<meta http-equiv="content-language" content="en">
		<meta name="author" content="Ettore Messina">	

		<style>
			#cookiescript_checkbox_input {
				-moz-appearance: checkbox;
				-webkit-appearance: checkbox;
				-ms-appearance: checkbox;
				appearance: checkbox;
				opacity: 1.0;
			}
			#cookiescript_checkbox_text {
				color: white;
			}
			#cookiescript_description a:hover {
				color: yellow !important;
			}
		</style>
		<script type="text/javascript" charset="UTF-8" src="https://cookie-script.com/s/19e1626ea9f21a6fcc285b559b5957e6.js"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance" src="https://www.googletagmanager.com/gtag/js?id=UA-149444322-1"></script>
		<script type="text/plain" data-cookiescript="accepted" data-cookiecategory="performance">
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'UA-149444322-1', { 'anonymize_ip': true });
		</script>


		<title>Fitting functions with a configurable XGBoost regressor</title>
		<meta name="description" content="Fitting of functions with a configurable XGBoost regressor" >
		<meta name="keywords" content="regression, xgboost, approximation, fit, fitting, function, dataset, machine learning" >
		<link rel="canonical" href="https://computationalmindset.com/en/machine-learning/fitting-with-configurable-xgboost.html" />
		<link rel="alternate" hreflang="en" href="https://computationalmindset.com/en/machine-learning/fitting-with-configurable-xgboost.html" />
		<link rel="alternate" hreflang="it" href="https://computationalmindset.com/it/machine-learning/approssimazione-di-funzioni-con-xgboost-configurabile.html" />
		
    <!-- SCHEMA.ORG JSON-LD WEBSITE -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "name": "Computational Mindset",
        "url": "https://computationalmindset.com/",
        "sameAs": ["https://www.facebook.com/ComputationalMindset/", "https://www.facebook.com/MentalitaComputazionale/", "https://github.com/ettoremessina/"],
        "author":
        {
          "@type": "Person",
          "name": "Ettore Messina",
          "image": "https://computationalmindset.com/images/ettore-messina.jpg",
          "gender": "Male",          
          "sameAs": ["https://www.facebook.com/ettore.messina.73/", "https://www.instagram.com/etmessina/", "https://twitter.com/ettoremessina/", "https://github.com/ettoremessina/", "https://medium.com/@ettoremessina/", "https://www.linkedin.com/in/ettoremessina/"]
        }
    }
    </script>

		
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement":
        [
		
          {
            "@type": "ListItem",
            "position": 1,
            "item":
            {
                "@id": "https://computationalmindset.com/it/",
                "name": "Mentalit&agrave; Computazionale"
            }
          },
		
          {
            "@type": "ListItem",
            "position": 2,
            "item":
            {
              "@id": "https://computationalmindset.com/it/machine-learning/",
              "name": "Machine Learning"
            }
          },

          {
            "@type": "ListItem",
            "position": 3,
            "item":
            {
              "@id": "https://computationalmindset.com/en/machine-learning/fitting-with-configurable-xgboost.html",
              "name": "Fitting functions with a configurable XGBoost regressor"
            }
		  }
		
        ]
    }
    </script>
		

		<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/railscasts.min.css">
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
		
		<script>hljs.initHighlightingOnLoad();</script>
		<style>
			pre > code 
			{
				font-size: 1.2em;
			}
		</style>

		
		<script type="text/javascript" src="https://latex.codecogs.com/latexit.js"></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>



		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
		<link rel="icon" href="../../favicon.ico" type="image/x-icon" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../en/info.html" class="logo"><strong>Computational&nbsp;Mindset</strong> by&nbsp;Ettore&nbsp;Messina</a>
									<div style="text-align:right">
										<a class="logo" href="../../en/">en</a>
										&nbsp;&nbsp;&nbsp;
										<a class="logo" href="../../it/">it</a>
									</div>
									<ul class="icons">
										<li><a href="https://github.com/ettoremessina/" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.facebook.com/ComputationalMindset/" class="icon brands fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
										<li><a href="https://www.youtube.com/channel/UCKrOtSEJjs5msOhPIdYEeWA/" class="icon brands fa-youtube" target="_blank"><span class="label">YouTube</span></a></li>
										<li><a href="https://www.linkedin.com/in/ettoremessina/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://medium.com/@ettoremessina/" class="icon brands fa-medium-m" target="_blank"><span class="label">Medium</span></a></li>
										<li><a href="https://linktr.ee/ComputationalMindset/" class="fas fa-link" style="color: grey;" target="_blank"><span class="label"></span></a></li>
									</ul>
								</header>

<!-- Content -->
	<section>
		<header class="main">
			<h1>Fitting functions with a configurable XGBoost regressor</h1>
		</header>

		<p>
			This post deals with the approximation of both scalar and vector real-valued mathematical functions to one or more real-valued variables using a <em>XGBoost</em> regressor
			without writing code but only acting on the command line of Python scripts that implement the functionality of:

			<ul>
				<li>
					<em>Regressor Configuration and training</em>
				</li>
				<li>
					<em>Prediction and error calculation</em>
				</li>
			</ul>

			The code described by this post requires Python version 3 and uses the XGBoost library; it also requires the SciKit-Learn, NumPy, Pandas, MatPlotLib and JobLib libraries.
			To get the code please see the paragraph <a href="#downloadcode">Download of the complete code</a> at the end of this post.<br/>
			<br/>
			For the generation of synthetic training and test datasets, the following common tools (available in the repository) will be used:
			<ul>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fx_gen"><code>fx_gen.py</code></a> for the real-valued scalar functions of one real-valued variable $f(x) \colon [a,b] \to {\rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fxy_gen"><code>fxy_gen.py</code></a> for the real-valued scalar functions of two real-valued variables $f(x,y) \colon [a,b] \times [c,d] \to {\rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#pmc2t_gen"><code>pcm2t_gen.py</code></a> for parametric curves on the plane, so real-valued vector functions $f(t) \colon [a,b] \to {\rm I\!R \times \rm I\!R}$
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#pmc3t_gen"><code>pmc3t_gen.py</code></a> for parametric curves in space, so real-valued vector functions $f(t) \colon [a,b] \to {\rm I\!R \times \rm I\!R \times \rm I\!R}$
				</li>
			</ul>			
			Also for the visualization of the results, and precisely for the comparison of the test dataset with the prediction, the following common tools (always available in the repository) will be used:
			<ul>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fx_scatter"><code>fx_scatter.py</code></a> for the real scalar generator functions of one real variable
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fxy_scatter"><code>fxy_scatter.py</code></a> for the real scalar generator functions of two real variables
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#pmc2t_scatter"><code>pmc2t_scatter.py</code></a> for parametric curves on the plane
				</li>
				<li>
					<a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#pmc3t_scatter"><code>pmc3t_scatter.py</code></a> for parametric curves in space
				</li>
			</ul>
		</p>

		<h2>Regressor Configuration and training</h2>
		<p>
			In this chapter the programs
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py"><code>fit_func_miso.py</code></a> and
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_mimo.py"><code>fit_func_mimo.py</code></a> are presented
			and they are technically wrappers of the class <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRFRegressor"><code>XGBRFRegressor</code></a>
			of the <em>XGBoost</em> library and which purpose is to allow the use of the regression of the underlying regressor to fit functions
			without having to write code but only acting on the command line.<br />
			In fact through the argument <code>--xgbparams</code> the user passes a series of hyper-parameters to adjust the behavior of the underlying XGBoost regressor algorithm
			and others to configure its learning phase.
			In addition to the parameters of the underlying regressor the two programs support their own arguments to allow the user to pass
			the training dataset and optionally the validation dataset, on which file to save the trained model, the metrics to calculate during the training,
			constraints for regularization (e.g. <em>early stop</em>) and parameters for diagnostic.<br />
			<br />
			The program <code>fit_func_miso.py</code>, as well as the underlying XGBoost regressor, is of type <em>M.I.S.O.</em>, i.e. <em>Multiple Input Single Output</em>:
			it is designed to fit a function of the form $f \colon \rm I\!R^n \to \rm I\!R$ where the number of independent variables is arbitrarily large
			while the output dependent variable is only one.<br />
			The format of the input datasets is in csv format (with header), with n + 1 columns, of which the first n ones contain the values of the n independent variables and
			the last column, the n+1, containing the values of the dependent variable.
			<br />
			The program <code>fit_func_mimo.py</code>, using in the implementation
			the <a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor"><code>sklearn.multioutput.MultiOutputRegressor</code></a> class,
			is of type <em>M.I.M.O.</em>, i.e. <em>Multiple Input multiple Output</em>:
			it is designed to fit a function of the form $f \colon \rm I\!R^n \to \rm I\!R^m$ where the number of independent variables is arbitrarily large
			while the output dependent variable is only one.<br />
			The format of the input datasets is in csv format (with header), with $n+m$ columns, of which the first $n$ columns contain the values of the $n$ independent variables and
			the last $m$ containing the values of the dependent variables.
		</p>

		<h3 id="fit_func_miso">Usage of the fit_func_miso.py program</h3>
		<p>
			To get the program usage you can run this following command:
			<pre><code class="shell">$ python fit_func_miso.py --help</code></pre>
			and the output got is:
			<br/>
			<br/>
			<pre><code class="shell">usage: fit_func_miso.py [-h] [--version] --trainds TRAIN_DATASET_FILENAME
                        --modelout MODEL_FILE [--valds VAL_DATASET_FILENAME]
                        [--metrics VAL_METRICS [VAL_METRICS ...]]
                        [--dumpout DUMPOUT_PATH]
                        [--earlystop EARLY_STOPPING_ROUNDS]
                        [--xgbparams XGB_PARAMS]

fit_func_miso.py fits a multiple-input single-output scalar function dataset
using a configurable XGBoost

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --trainds TRAIN_DATASET_FILENAME
                        Train dataset file (csv format)
  --modelout MODEL_FILE
                        Output model file
  --valds VAL_DATASET_FILENAME
                        Validation dataset file (csv format)
  --metrics VAL_METRICS [VAL_METRICS ...]
                        List of built-in evaluation metrics to apply to
                        validation dataset
  --dumpout DUMPOUT_PATH
                        Dump directory (directory to store metric values)
  --earlystop EARLY_STOPPING_ROUNDS
                        Number of round for early stopping
  --xgbparams XGB_PARAMS
                        Parameters of XGBoost constructor</code></pre>
			Where:
			<ul>
				<li>
					<b>-h, --help</b>: shows the usage of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: shows the version of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--trainds</b>: path (relative or absolute) of a two-column csv file (with header) that contains the dataset to be used for the training;
					this file can be generated synthetically e.g. via the program <a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fx_gen"><code>fx_gen.py</code></a>.
					or be a dataset actually obtained by measuring a scalar and real phenomenon that depends on a single real-valued variable.<br />
					<br />
				</li>
				<li>
					<b>--modelout</b>: path (relative or absolute) to a file where to save the trained model in joblib format (.jl).<br />
					<br />
				</li>
				<li>
					<b>--valds</b>: path (relative or absolute) of a two-column csv file (with header) that contains the dataset to be used for validation.<br />
					<br />
				</li>
				<li>
					<b>--metrics</b>: list of metrics to be calculated on the training dataset and, if present, also on the validation dataset;
					the list of supported metrics is defined in <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/parameter.html">XGBoost Parameters</a>.
					under <em>eval_metric</em>.<br />
					<br />
				</li>
				<li>
					<b>--dumpout</b>: path (relative or absolute) of the directory where to save the metric values as the ages go by;
					the program <a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#dumps_scatter"><code>dumps_scatter.py</code></a> will use the contents of this directory to display the metric graphs.<br />
					<br />
				</li>
				<li>
					<b>--earlystop</b>: how many iterations can be performed before the algorithm begins to enter the overfitting phase.<br />
					<br />
				</li>
				<li>
					<b>--xgbparams</b>: list of parameters to pass to XGBoost regression algorithm; see documentation of <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"><code>XGBRegressor</code></a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h3 id="fit_func_mimo">Usage of the fit_func_mism.py program</h3>
		<p>
			To get the program usage you can run this following command:
			<pre><code class="shell">$ python fit_func_mimo.py --help</code></pre>
			and the output got is:
			<br/>
			<br/>
			<pre><code class="shell">usage: fit_func_mimo.py [-h] [--version] --trainds TRAIN_DATASET_FILENAME
                        --outputdim NUM_OF_DEPENDENT_COLUMNS --modelout
                        MODEL_FILE [--dumpout DUMPOUT_PATH]
                        [--xgbparams XGB_PARAMS]

fit_func_mimo.py fits a multiple-input single-output (scalar) function dataset
using a configurable XGBoost

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --trainds TRAIN_DATASET_FILENAME
                        Train dataset file (csv format)
  --outputdim NUM_OF_DEPENDENT_COLUMNS
                        Output dimension (alias the number of dependent
                        columns, that must be last columns)
  --modelout MODEL_FILE
                        Output model file
  --dumpout DUMPOUT_PATH
                        Dump directory (directory to store metric values)
  --xgbparams XGB_PARAMS
                        Parameters of XGBoost constructor</code></pre>
			Where:
			<ul>
				<li>
					<b>-h, --help</b>: shows the usage of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: shows the version of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--trainds</b>: path (relative or absolute) of a two-column csv file (with header) that contains the dataset to be used for the training;
					this file can be generated synthetically e.g. via the program <a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#fx_gen"><code>fx_gen.py</code></a>.
					or be a dataset actually obtained by measuring a scalar and real phenomenon that depends on a single real-valued variable.<br />
					<br />
				</li>
				<li>
					<b>--outputdim</b>:
						the $n$ number of independent variables that are the first $n$ columns of the csv dataset;
						the rest of the columns on the right are the $m$ dependent variables accordingly.<br />
					<br />
				</li>
				<li>
					<b>--modelout</b>: path (relative or absolute) to a file where to save the trained model in joblib format (.jl).<br />
					<br />
				</li>
				<li>
					<b>--dumpout</b>: path (relative or absolute) of the directory where to save the metric values as the ages go by;
					the program <a target="_blank" href="../../en/machine-learning/common-tools-for-function-fitting.html#dumps_scatter"><code>dumps_scatter.py</code></a> will use the contents of this directory to display the metric graphs.<br />
					<br />
				</li>
				<li>
					<b>--xgbparams</b>: list of parameters to pass to XGBoost regression algorithm; see documentation of <a target="_blank" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"><code>XGBRegressor</code></a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h2>Prediction and error calculation</h2>
		<p>
			In this chapter the programs
			<a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/predict_func_miso.py" target="_blank"><code>predict_func_miso.py</code></a> and
			<a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/predict_func_miso.py" target="_blank"><code>predict_func_miso.py</code></a> are presented
			and which purpose is to make predictions on a test dataset applying it to a previously trained XGBoost regressor model
			respectively via the programs
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py"><code>fit_func_miso.py</code></a> and
			<a target="_blank" href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_mimo.py"><code>fit_func_mimo.py</code></a>,
			always without having to write code but only through the command line.<br />
			In fact, the two programs support arguments through which the user passes the previously trained model, the test dataset
			and the error measurements to be calculated between the predictions and the true values.<br />
			The format of the incoming test datasets is identical to the ones rispectively of the programs <code>fit_func_miso.py</code> and <code>fit_func_mimo.py</code>;
			of course the last columns (related to the dependent variables) are only used to compare the predicted values with the true values by calculating passed error measurements.<br />
		</p>

		<h3 id="predict_func_miso">Usage of the predict_func_miso.py program</h3>
		<p>
			To get the program usage you can run this following command:
			<pre><code class="shell">$ python predict_func_miso.py --help</code></pre>
			and the output got is:
			<br/>
			<br/>
			<pre><code class="shell">usage: predict_func_miso.py [-h] [--version] --model MODEL_FILE --ds
                            DF_PREDICTION --predictionout PREDICTION_DATA_FILE
                            [--measures MEASURES [MEASURES ...]]

predict_func_miso.py makes prediction of the values of a multiple-input
single-output (scalar) function with a pretrained XGBoost model

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --model MODEL_FILE    model file
  --ds DF_PREDICTION    dataset file (csv format)
  --predictionout PREDICTION_DATA_FILE
                        prediction data file (csv format)
  --measures MEASURES [MEASURES ...]
                        List of built-in sklearn regression metrics to compare
                        prediction with input dataset</code></pre>
			Where:
			<ul>
				<li>
					<b>-h, --help</b>: shows the usage of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: shows the version of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--model</b>: path (relative or absolute) to the file in joblib (.jl) format of the model generated by <code>fit_func_miso.py</code>.<br />
					<br />
				</li>
				<li>
					<b>--ds</b>: path (relative or absolute) of the csv file (with header) that contains the input dataset on which to calculate the prediction.<br />
					<br />
				</li>
				<li>
					<b>--predictionout</b>: path (relative or absolute) of the csv file to generate that will contain the prediction, that is the approximation of the function applied to the input dataset.<br />
					<br />
				</li>
				<li>
					<b>--measures</b>: list of measurements to be calculated by comparing the true values of the input dataset and the predicted output values;
					the list of supported metrics is defined in <a target="_blank" href="https://scikit-learn.org/stable/modules/classes.html#regression-metrics">SciKit Learn Regression Metrics</a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h3 id="predict_func_mimo">Usage of the predict_func_mimo.py program</h3>
		<p>
			To get the program usage you can run this following command:
			<pre><code class="shell">$ python predict_func_mimo.py --help</code></pre>
			and the output got is:
			<br/>
			<br/>
			<pre><code class="shell">usage: predict_func_mimo.py [-h] [--version] --model MODEL_FILE --ds
                            DF_PREDICTION --outputdim NUM_OF_DEPENDENT_COLUMNS
                            --predictionout PREDICTION_DATA_FILE
                            [--measures MEASURES [MEASURES ...]]

predict_func_mimo.py makes prediction of the values of a multiple-input
single-output (scalar) function with a pretrained XGBoost model

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&apos;s version number and exit
  --model MODEL_FILE    model file
  --ds DF_PREDICTION    dataset file (csv format)
  --outputdim NUM_OF_DEPENDENT_COLUMNS
                        Output dimension (alias the number of dependent
                        columns, that must be last columns)
  --predictionout PREDICTION_DATA_FILE
                        prediction data file (csv format)
  --measures MEASURES [MEASURES ...]
                        List of built-in sklearn regression metrics to compare
                        prediction with input dataset</code></pre>
			Where:
			<ul>
				<li>
					<b>-h, --help</b>: shows the usage of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--version</b>: shows the version of the program and ends the execution.<br />
					<br />
				</li>
				<li>
					<b>--model</b>: path (relative or absolute) to the file in joblib (.jl) format of the model generated by <code>fit_func_miso.py</code>.<br />
					<br />
				</li>
				<li>
					<b>--ds</b>: path (relative or absolute) of the csv file (with header) that contains the input dataset on which to calculate the prediction.<br />
					<br />
				</li>
				<li>
					<b>--outputdim</b>:
						the $n$ number of independent variables that are the first $n$ columns of the csv dataset;
						the rest of the columns on the right are the $m$ dependent variables accordingly.<br />
					<br />
				</li>
				<li>
					<b>--predictionout</b>: path (relative or absolute) of the csv file to generate that will contain the prediction, that is the approximation of the function applied to the input dataset.<br />
					<br />
				</li>
				<li>
					<b>--measures</b>: list of measurements to be calculated by comparing the true values of the input dataset and the predicted output values;
					the list of supported metrics is defined in <a target="_blank" href="https://scikit-learn.org/stable/modules/classes.html#regression-metrics">SciKit Learn Regression Metrics</a>.<br />
					<br />
				</li>
			</ul>
		</p>

		<h2>An example of using of all the programs</h3>
		<p>
			Suppose you want to approximate the function $$f(x)=x sin \frac{1}{x}$$ in the range $[-0.4,0.4]$ 
			Keeping in mind that <em>np</em> is the alias of NumPy library, the translation of this function in lambda body Python syntax is:
			<pre><code class="python">x * np.sin(1 / x)</code></pre>
			To generate the training dataset, run the following command:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_gen.py \
  --dsout mytrain.csv \
  --funcx "x * np.sin(1 / x)" \
  --xbegin -0.4 \
  --xend 0.4 \
  --xstep 0.00031</code></pre>
			instead to generate the test dataset, run the following command:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_gen.py \
  --dsout mytest.csv \
  --funcx "x * np.sin(1 / x)" \
  --xbegin -0.4 \
  --xend 0.4 \
  --xstep 0.00073</code></pre>
			Note that the discretization step of the test dataset is larger than that of training and it is a normal fact
			because the training, to be accurate, it must be run on more data.
			Also note that it is appropriate for the discretization step of the test dataset is <u>not</u> a multiple of the training one
			in order to ensure that the test dataset contains most of the data that is not present in training dataset, and this makes prediction more interesting.
			<br />
			To this we intend to make a regression by <a href="https://github.com/ettoremessina/function-fitting/blob/master/xgboost/fit_func_miso.py" target="_blank"><code>fit_func_miso.py</code></a>
			passing to the underlying regressor: n_estimators: 100, max_depth: 7;
			then run the following command:
			<br/>
			<br/>
			<pre><code class="shell">$ python fit_func_miso.py \
  --trainds mytrain.csv \
  --modelout mymodel.jl \
  --xgbparams "'n_estimators': 100, 'max_depth': 7"</code></pre>
			and at the end of the execution the saved mymodel.jl file contains the model of the XGBoost regressor configured and trained.<br />
			<br />
			Now we intend to perform the prediction and calculation of the error using the measurements <em>mean_absolute_error</em> and <em>mean_squared_error</em>;
			then execute the following command:
			<br/>
			<br/>
			<pre><code class="shell">$ python predict_func_miso.py \
  --model mymodel.jl \
  --ds mytest.csv \
  --predictionout mypred.csv \
  --measures mean_absolute_error mean_squared_error</code></pre>
  			and at the end of the execution the saved mypred.csv file contains the prediction performed by applying the model on the test data;
			the output of the program displays the error measures passed through the argument <code>--measures</code>
			and they are very small: the first one around $1.5 \cdot 10^{-3}$ and the second one around $5.5 \cdot 10^{-6}$<br />
			<b>Note</b>: Given the stochastic nature of the training phase, your specific results may vary. Consider running the training phase a few times.
			<br />
			Finally you want to make the comparative display of the test dataset with the prediction;
			therefore run the following command:
			<br/>
			<br/>
			<pre><code class="shell">$ python fx_scatter.py \
  --ds mytest.csv \
  --prediction mypred.csv \
  --title "XGBoost (estimators: 100, max depth: 7)" \
  --xlabel "x" \
  --ylabel "y=x sin(1/x)"</code></pre>
			that shows the dispersion graphs of the test dataset and the superimposed prediction: in blue the one of the test dataset, in red the prediction.
			The comparison of the two graphs clearly shows that the approximation has reached very high levels, as the low error measurements already indicated;
			only around zero is some inaccuracy observed and is due to the oscillation of the function that is there very strong.<br />
			<br />
			<b>Note</b>: Given the stochastic nature of the training phase, your specific results may vary. Consider running the training phase a few times.
		</p>
		<div class="betweentextlines"><img src="../../posts/machine-learning/fitting-with-configurable-xgboost/ml-fitfnwcxgb-example10.png" /></div>
		<div class="photocaption">Figure with dispersion graphs generated by the program <code>fx_scatter.py</code> showing the fitting in red overlay of the function $f(x)=x sin \frac{1}{x}$
		and the original function below in blue.</div>
		<br />
		<p>
			The repository contains examples in shell scripts that show the use of these cascading programs:
			<ul>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/one-variable-function/xgboost/examples"><code>one-variable-function/xgboost/examples</code></a>
					for examples on the fitting of real-valued scalar functions of one real-valued variable via <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/two-variables-function/xgboost/examples"><code>two-variables-function/xgboost/examples</code></a> p
					for examples on the fitting of real-valued scalar functions of two real-valued variables via <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/parametric-curve-on-plane/xgboost/examples"><code>parametric-curve-on-plane/xgboost/examples</code></a>
					for examples about fitting parametric curves on the plane via <em>XGBoost</em>.
				</li>
				<li>
					<a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/parametric-curve-in-space/xgboost/examples"><code>parametric-curve-in-space/xgboost/examples</code></a>
					for examples about fitting parametric curves on the space via <em>XGBoost</em>.
				</li>
			</ul>
		</p>
		<br/>

		<h2 id="downloadcode">Download of the complete code</h2>
		<p>
			The complete code is available at <a target="_blank" href="https://github.com/ettoremessina/function-fitting/tree/master/xgboost/">GitHub</a>.
			<br/>
			
			These materials are distributed under MIT license; feel free to use, share, fork and adapt these materials as you see fit.
			<br/>
			Also please feel free to submit pull-requests and bug-reports to this GitHub repository or contact me on my social media channels available on the top right corner of this page.
			<br/>

		</p>
	</section>

						</div>
				</div>
				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<div class="align-center"><img src="../../images/cm-logo-small.png" alt="Computational&nbsp;Mindset" /></div>
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="../../en/">Home</a></li>
										<li>
											<span class="opener">Neural&nbsp;Networks</span>
											<ul>
												<li><a href="../../en/neural-networks/">INDEX</a></li>
												<li><a href="../../en/neural-networks/differential-equations-and-neural-networks.html">Differential Equations and Neural Networks</a></li>
												<li><a href="../../en/neural-networks/univariate-equally-spaced-time-series-forecast-with-tensorflow.html">Forecast of a univariate equally spaced time series with TensorFlow</a></li>
												<li><a href="../../en/neural-networks/fitting-with-multi-layer-perceptrons-highly-configurable.html">Fitting with highly configurable multi layer perceptrons</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Quantum&nbsp;Computing</span>
											<ul>
												<li><a href="../../en/quantum-computing/">INDEX</a></li>
												<li><a href="../../en/quantum-computing/not-cnot-operators.html">NOT and C-NOT quantum gates</a></li>
												<li><a href="../../en/quantum-computing/random-number-generation.html">Random Numbers Generation</a></li>
												<li><a href="../../en/quantum-computing/hadamard-gate-cascade.html">Cascade Hadamard Gates</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Machine&nbsp;Learning</span>
											<ul>
												<li><a href="../../en/machine-learning/">INDEX</a></li>
												<li><a href="../../en/machine-learning/fitting-with-configurable-xgboost.html">Fitting functions with a configurable XGBoost regressor</a></li>
												<li><a href="../../en/machine-learning/fitting-with-configurable-svr.html">Fitting functions with a configurable Support Vector Regressor</a></li>
												<li><a href="../../en/machine-learning/polynomial-regression-with-accord-net.html">Polynomial regression with Accord.NET</a></li>
												<li><a href="../../en/machine-learning/smo-regression-with-puk-kernel-in-weka.html">SMO regression for SVM with PUK kernel in Weka</a></li>
												<li><a href="../../en/machine-learning/smo-forecast-with-poly-kernel-in-weka.html">SMO forecast for SVM with polynomial kernel in Weka</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Mathematics</span>
											<ul>
												<li><a href="../../en/mathematics/">INDEX</a></li>
												<li><a href="../../en/mathematics/analyzer-of-a-constant-coefficient-linear-and-homogeneous-dynamical-system-on-plane.html">Analyzer of a constant coefficient linear and homogeneous dynamical system on plane</a></li>
												<li><a href="../../en/mathematics/experiments-with-sympy-to-solve-odes-1st-order.html">Experiments with SymPy to solve first-order ordinary differential equations</a></li>
												<li><a href="../../en/mathematics/method-solving-first-order-dde-using-lambert-w-function.html">A method to solve first-order time delayed differential equation using Lambert W function</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Datasets</span>
											<ul>
												<li><a href="../../en/datasets/">INDEX</a></li>
												<li><a href="../../en/datasets/functions-dataset.html">&apos;Functions&apos; dataset collection</a></li>
												<li><a href="../../en/datasets/time-series-dataset.html">&apos;Time&nbsp;Series&apos; dataset collection</a></li>
												<li><a href="../../en/datasets/synthetic-words-dataset.html">&apos;Synthetic Words&apos; dataset</a></li>
											</ul>
										</li>
										<li><a href="../../en/info.html">Info</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
                                    <div class="side-topics">
                                        <header class="align-center">
                                            <h2><a href="../../en/neural-networks/">Neural&nbsp;Networks</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../en/neural-networks/" class="image"><span class="icon solid fa-sitemap"/></a>
                                        </article>
                                        <header class="align-center">
                                            <h2><a href="../../en/quantum-computing/">Quantum&nbsp;Computing</a></h2>
                                        </header>
                                        <article>
                                            <a href="../../en/quantum-computing/" class="image"><span class="icon solid fa-atom" /></a>
                                        </article>
                                    </div>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">
										Design based on &apos;Editorial&apos; template (with customization) downloaded from <a href="https://html5up.net" target="_blank">HTML5 UP</a>.
										<br/>
										Click on links to see <a href="../../html5up-license/LICENSE.txt" target="_blank">LICENSE.txt</a> and <a href="../../html5up-license/README.txt" target="_blank">README.txt</a> files of &apos;Editorial&apos; template by HTML5 UP.
										<br>
										<br>
										&copy; <a href="../../en/info.html">Ettore Messina</a>. 
									</p>
								</footer>
						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

			<style>
				a.cc-link
				{
				    border-bottom: none;
				}
				a.cc-link:hover
				{
					color: white !important;
				}
			</style>
	</body>
</html>

